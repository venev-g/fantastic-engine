{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e97602d5",
                "language": "markdown"
            },
            "source": [
                "# Advanced Intent Recognition and Entity Extraction\n",
                "\n",
                "This notebook demonstrates production-grade techniques for NLU using modern machine learning approaches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting transformers\n",
                        "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
                        "Collecting datasets\n",
                        "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
                        "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.6.1)\n",
                        "Collecting nltk\n",
                        "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
                        "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
                        "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
                        "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
                        "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (24.2)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
                        "Collecting regex!=2019.12.17 (from transformers)\n",
                        "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
                        "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
                        "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
                        "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
                        "Collecting safetensors>=0.4.3 (from transformers)\n",
                        "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
                        "Collecting tqdm>=4.27 (from transformers)\n",
                        "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
                        "Collecting pyarrow>=15.0.0 (from datasets)\n",
                        "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
                        "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
                        "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
                        "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
                        "Collecting xxhash (from datasets)\n",
                        "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
                        "Collecting multiprocess<0.70.17 (from datasets)\n",
                        "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
                        "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
                        "Collecting aiohttp (from datasets)\n",
                        "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
                        "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
                        "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
                        "Collecting click (from nltk)\n",
                        "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
                        "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
                        "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
                        "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
                        "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
                        "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
                        "  Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
                        "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
                        "  Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
                        "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
                        "  Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
                        "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
                        "  Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
                        "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
                        "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
                        "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
                        "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
                        "Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
                        "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
                        "Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
                        "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
                        "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
                        "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
                        "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
                        "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
                        "Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
                        "Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
                        "Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
                        "Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
                        "Installing collected packages: xxhash, tqdm, safetensors, regex, pyarrow, propcache, multidict, frozenlist, dill, click, aiohappyeyeballs, yarl, nltk, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
                        "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 click-8.1.8 datasets-3.5.1 dill-0.3.8 frozenlist-1.6.0 huggingface-hub-0.30.2 multidict-6.4.3 multiprocess-0.70.16 nltk-3.9.1 propcache-0.3.1 pyarrow-20.0.0 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3 xxhash-3.5.0 yarl-1.20.0\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "# Install required libraries\n",
                "!pip install transformers datasets scikit-learn nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
                "from transformers import pipeline\n",
                "import torch\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "323665d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import tqdm as notebook_tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Intent Recognition with Transformer Models\n",
                "\n",
                "Production systems use pre-trained language models fine-tuned on domain-specific data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.microsoft.datawrangler.viewer.v0+json": {
                            "columns": [
                                {
                                    "name": "index",
                                    "rawType": "int64",
                                    "type": "integer"
                                },
                                {
                                    "name": "text",
                                    "rawType": "object",
                                    "type": "string"
                                },
                                {
                                    "name": "intent",
                                    "rawType": "object",
                                    "type": "string"
                                }
                            ],
                            "conversionMethod": "pd.DataFrame",
                            "ref": "f18afe8d-bce8-47bb-bf60-5df2fa1f6bb1",
                            "rows": [
                                [
                                    "8",
                                    "I want to order a pizza",
                                    "order_food"
                                ],
                                [
                                    "11",
                                    "I'd like to order dinner",
                                    "order_food"
                                ],
                                [
                                    "6",
                                    "Tell me the forecast for this weekend",
                                    "weather"
                                ],
                                [
                                    "1",
                                    "Can you reserve a table for two on Friday?",
                                    "booking"
                                ],
                                [
                                    "7",
                                    "How hot will it be today?",
                                    "weather"
                                ],
                                [
                                    "0",
                                    "I want to book a flight to New York",
                                    "booking"
                                ],
                                [
                                    "5",
                                    "Is it going to rain tomorrow?",
                                    "weather"
                                ],
                                [
                                    "10",
                                    "Order me some takeout",
                                    "order_food"
                                ],
                                [
                                    "3",
                                    "Book me a taxi for tomorrow morning",
                                    "booking"
                                ],
                                [
                                    "9",
                                    "Can I get a large pepperoni with extra cheese?",
                                    "order_food"
                                ]
                            ],
                            "shape": {
                                "columns": 2,
                                "rows": 10
                            }
                        },
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>text</th>\n",
                            "      <th>intent</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>I want to order a pizza</td>\n",
                            "      <td>order_food</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>11</th>\n",
                            "      <td>I'd like to order dinner</td>\n",
                            "      <td>order_food</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>Tell me the forecast for this weekend</td>\n",
                            "      <td>weather</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>Can you reserve a table for two on Friday?</td>\n",
                            "      <td>booking</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>How hot will it be today?</td>\n",
                            "      <td>weather</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>I want to book a flight to New York</td>\n",
                            "      <td>booking</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>Is it going to rain tomorrow?</td>\n",
                            "      <td>weather</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>10</th>\n",
                            "      <td>Order me some takeout</td>\n",
                            "      <td>order_food</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Book me a taxi for tomorrow morning</td>\n",
                            "      <td>booking</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>Can I get a large pepperoni with extra cheese?</td>\n",
                            "      <td>order_food</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                              text      intent\n",
                            "8                          I want to order a pizza  order_food\n",
                            "11                        I'd like to order dinner  order_food\n",
                            "6            Tell me the forecast for this weekend     weather\n",
                            "1       Can you reserve a table for two on Friday?     booking\n",
                            "7                        How hot will it be today?     weather\n",
                            "0              I want to book a flight to New York     booking\n",
                            "5                    Is it going to rain tomorrow?     weather\n",
                            "10                           Order me some takeout  order_food\n",
                            "3              Book me a taxi for tomorrow morning     booking\n",
                            "9   Can I get a large pepperoni with extra cheese?  order_food"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Create sample intent data\n",
                "intent_examples = {\n",
                "    'booking': [\n",
                "        \"I want to book a flight to New York\",\n",
                "        \"Can you reserve a table for two on Friday?\",\n",
                "        \"I need to schedule an appointment\",\n",
                "        \"Book me a taxi for tomorrow morning\"\n",
                "    ],\n",
                "    'weather': [\n",
                "        \"What's the weather like in London?\",\n",
                "        \"Is it going to rain tomorrow?\",\n",
                "        \"Tell me the forecast for this weekend\",\n",
                "        \"How hot will it be today?\"\n",
                "    ],\n",
                "    'order_food': [\n",
                "        \"I want to order a pizza\",\n",
                "        \"Can I get a large pepperoni with extra cheese?\",\n",
                "        \"Order me some takeout\",\n",
                "        \"I'd like to order dinner\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "# Convert to DataFrame format for training\n",
                "texts = []\n",
                "labels = []\n",
                "\n",
                "for intent, examples in intent_examples.items():\n",
                "    for example in examples:\n",
                "        texts.append(example)\n",
                "        labels.append(intent)\n",
                "\n",
                "intent_df = pd.DataFrame({'text': texts, 'intent': labels})\n",
                "intent_df.sample(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using Pre-trained Transformer Model for Intent Classification\n",
                "\n",
                "In production, we would fine-tune a model on thousands of examples, but we'll use a pre-trained model to demonstrate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a pre-trained zero-shot classification model\n",
                "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
                "\n",
                "# Function to classify intents using the model\n",
                "def classify_intent_transformer(text, candidate_labels):\n",
                "    result = classifier(text, candidate_labels)\n",
                "    return {\n",
                "        \"intent\": result['labels'][0],\n",
                "        \"confidence\": result['scores'][0],\n",
                "        \"all_intents\": dict(zip(result['labels'], result['scores']))\n",
                "    }\n",
                "\n",
                "# Example\n",
                "sample_text = \"I want to reserve a table for two at an Italian restaurant\"\n",
                "intent_result = classify_intent_transformer(sample_text, list(intent_examples.keys()))\n",
                "print(f\"Text: {sample_text}\")\n",
                "print(f\"Detected Intent: {intent_result['intent']} (confidence: {intent_result['confidence']:.4f})\")\n",
                "print(\"All intents with confidence scores:\")\n",
                "for intent, score in intent_result['all_intents'].items():\n",
                "    print(f\"  - {intent}: {score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Named Entity Recognition with Transformer Models\n",
                "\n",
                "Production systems use specialized token classification models for entity extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a pre-trained NER model\n",
                "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
                "\n",
                "def extract_entities_transformer(text):\n",
                "    entities = ner_pipeline(text)\n",
                "    \n",
                "    # Format the results\n",
                "    formatted_entities = {}\n",
                "    for entity in entities:\n",
                "        entity_type = entity['entity_group']\n",
                "        if entity_type in formatted_entities:\n",
                "            if isinstance(formatted_entities[entity_type], list):\n",
                "                formatted_entities[entity_type].append(entity['word'])\n",
                "            else:\n",
                "                formatted_entities[entity_type] = [formatted_entities[entity_type], entity['word']]\n",
                "        else:\n",
                "            formatted_entities[entity_type] = entity['word']\n",
                "    \n",
                "    return formatted_entities\n",
                "\n",
                "# Example\n",
                "entity_texts = [\n",
                "    \"I want to fly from New York to San Francisco next Friday\",\n",
                "    \"John Smith works at Microsoft in Seattle\",\n",
                "    \"I need to meet Sarah Johnson at the coffee shop on Main Street\"\n",
                "]\n",
                "\n",
                "for text in entity_texts:\n",
                "    entities = extract_entities_transformer(text)\n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"Extracted Entities: {entities}\")\n",
                "    print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Combined Intent and Entity Processing\n",
                "\n",
                "In production systems, intent recognition and entity extraction work together in a pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_text_production(text, intent_labels):\n",
                "    # Step 1: Intent Recognition\n",
                "    intent_result = classify_intent_transformer(text, intent_labels)\n",
                "    \n",
                "    # Step 2: Entity Extraction\n",
                "    entities = extract_entities_transformer(text)\n",
                "    \n",
                "    # Step 3: Combine Results\n",
                "    return {\n",
                "        \"text\": text,\n",
                "        \"intent\": intent_result[\"intent\"],\n",
                "        \"intent_confidence\": intent_result[\"confidence\"],\n",
                "        \"entities\": entities\n",
                "    }\n",
                "\n",
                "# Testing with complex examples\n",
                "test_examples = [\n",
                "    \"Book me a flight from London to Paris for next Tuesday\",\n",
                "    \"What's the weather going to be like in New York this weekend?\",\n",
                "    \"I'd like to order a large pepperoni pizza with extra cheese from Domino's\"\n",
                "]\n",
                "\n",
                "for example in test_examples:\n",
                "    result = analyze_text_production(example, list(intent_examples.keys()))\n",
                "    print(f\"Text: {result['text']}\")\n",
                "    print(f\"Intent: {result['intent']} (confidence: {result['intent_confidence']:.4f})\")\n",
                "    print(f\"Entities: {result['entities']}\")\n",
                "    print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Fine-tuning Process for Production Systems\n",
                "\n",
                "In real-world applications, these models would be fine-tuned on domain-specific data:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pseudo-code for fine-tuning process (not meant to be executed)\n",
                "\"\"\"\n",
                "# 1. Data collection and annotation\n",
                "- Collect thousands of examples specific to your domain\n",
                "- Have human annotators label intents and entities\n",
                "- Split into train/validation/test sets\n",
                "\n",
                "# 2. Intent classifier fine-tuning\n",
                "from transformers import Trainer, TrainingArguments\n",
                "from datasets import Dataset\n",
                "\n",
                "# Convert data to HuggingFace datasets\n",
                "train_dataset = Dataset.from_pandas(train_df)\n",
                "eval_dataset = Dataset.from_pandas(eval_df)\n",
                "\n",
                "# Load pre-trained model\n",
                "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(intents))\n",
                "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
                "\n",
                "# Fine-tune\n",
                "training_args = TrainingArguments(\n",
                "    output_dir='./results',\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=64,\n",
                "    weight_decay=0.01,\n",
                "    logging_dir='./logs',\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=eval_dataset\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "\n",
                "# 3. NER model fine-tuning (similar process)\n",
                "\n",
                "# 4. Deploy models to production\n",
                "- Export to ONNX or TorchScript for faster inference\n",
                "- Set up monitoring for model performance\n",
                "- Implement continuous learning and model updating\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Advanced Techniques Used in Production\n",
                "\n",
                "1. **Few-shot and zero-shot learning** - Handling new intents without extensive training\n",
                "2. **Confidence thresholds** - Falling back to default behaviors when uncertain\n",
                "3. **Multi-intent detection** - Recognizing multiple intents in a single utterance\n",
                "4. **Context tracking** - Maintaining conversation state across multiple turns\n",
                "5. **Hybrid approaches** - Combining rule-based systems with ML for high reliability\n",
                "6. **Active learning** - Continuously improving models from user interactions"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
