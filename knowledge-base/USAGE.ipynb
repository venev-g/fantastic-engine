{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a3f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/fantastic-engine/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Any, Optional, Union\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    HTMLHeaderTextSplitter\n",
    ")\n",
    "from langchain_community.vectorstores.milvus import Milvus\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Document processing\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "import tabula\n",
    "\n",
    "# Google Gemini\n",
    "import google.generativeai as genai\n",
    "from tqdm.auto import tqdm as notebook_tqdm\n",
    "print(\"All required libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fde16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAnalyzer:\n",
    "    \"\"\"Class to analyze document structure and extract various elements.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_document(self, file_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Load document from file path and return structured content.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with structured document content\n",
    "        \"\"\"\n",
    "        # Get file extension\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "        ext = ext.lower()\n",
    "\n",
    "        if ext == '.pdf':\n",
    "            return self._process_pdf(file_path)\n",
    "        elif ext in ['.html', '.htm']:\n",
    "            return self._process_html(file_path)\n",
    "        elif ext in ['.txt', '.md']:\n",
    "            return self._process_text(file_path)\n",
    "        elif ext in ['.docx', '.doc']:\n",
    "            return self._process_word(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "    def _process_pdf(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process PDF documents.\"\"\"\n",
    "        document_structure = {\n",
    "            'metadata': {'source': file_path, 'type': 'pdf'},\n",
    "            'elements': []\n",
    "        }\n",
    "\n",
    "        # Read PDF and extract text\n",
    "        elements = partition_pdf(\n",
    "            filename=file_path,\n",
    "            extract_images=True,\n",
    "            infer_table_structure=True\n",
    "        )\n",
    "\n",
    "        # Extract tables separately using tabula\n",
    "        tables = tabula.read_pdf(file_path, pages='all')\n",
    "\n",
    "        # Process elements and categorize them\n",
    "        for element in elements:\n",
    "            elem_type = str(type(element)).lower()\n",
    "            element_data = {\n",
    "                'content': str(element),\n",
    "                'type': None\n",
    "            }\n",
    "\n",
    "            if 'title' in elem_type or 'heading' in elem_type:\n",
    "                element_data['type'] = 'heading'\n",
    "            elif 'table' in elem_type:\n",
    "                element_data['type'] = 'table'\n",
    "            elif 'image' in elem_type:\n",
    "                element_data['type'] = 'image' \n",
    "            elif 'text' in elem_type:\n",
    "                # Check if it's strikeout or highlighted (would need more PDF-specific analysis)\n",
    "                if '~~' in str(element) or '--' in str(element):\n",
    "                    element_data['type'] = 'strikeout'\n",
    "                elif any(marker in str(element) for marker in ['**', '__', '>>']):\n",
    "                    element_data['type'] = 'highlight'\n",
    "                else:\n",
    "                    element_data['type'] = 'paragraph'\n",
    "\n",
    "            document_structure['elements'].append(element_data)\n",
    "\n",
    "        # Add tables from tabula to our elements list\n",
    "        for i, table in enumerate(tables):\n",
    "            document_structure['elements'].append({\n",
    "                'content': table,\n",
    "                'type': 'table',\n",
    "                'pandas_table': True,\n",
    "                'table_id': i\n",
    "            })\n",
    "\n",
    "        return document_structure\n",
    "\n",
    "    def _process_html(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process HTML documents.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        document_structure = {\n",
    "            'metadata': {'source': file_path, 'type': 'html'},\n",
    "            'elements': []\n",
    "        }\n",
    "\n",
    "        # Extract headings\n",
    "        for heading_level in range(1, 7):\n",
    "            for heading in soup.find_all(f'h{heading_level}'):\n",
    "                document_structure['elements'].append({\n",
    "                    'content': heading.get_text(),\n",
    "                    'type': 'heading',\n",
    "                    'level': heading_level\n",
    "                })\n",
    "\n",
    "        # Extract paragraphs\n",
    "        for para in soup.find_all('p'):\n",
    "            # Check for highlighted text\n",
    "            highlighted = para.find_all(['strong', 'b', 'mark', 'em'])\n",
    "            strikeout = para.find_all('s')\n",
    "\n",
    "            if highlighted:\n",
    "                for h in highlighted:\n",
    "                    document_structure['elements'].append({\n",
    "                        'content': h.get_text(),\n",
    "                        'type': 'highlight',\n",
    "                    })\n",
    "\n",
    "            if strikeout:\n",
    "                for s in strikeout:\n",
    "                    document_structure['elements'].append({\n",
    "                        'content': s.get_text(),\n",
    "                        'type': 'strikeout',\n",
    "                    })\n",
    "\n",
    "            # Add the full paragraph too\n",
    "            document_structure['elements'].append({\n",
    "                'content': para.get_text(),\n",
    "                'type': 'paragraph',\n",
    "            })\n",
    "\n",
    "        # Extract tables\n",
    "        for table in soup.find_all('table'):\n",
    "            # Convert HTML table to pandas DataFrame\n",
    "            table_data = []\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cols = row.find_all(['td', 'th'])\n",
    "                cols = [ele.get_text().strip() for ele in cols]\n",
    "                table_data.append(cols)\n",
    "\n",
    "            if table_data:\n",
    "                # Try to create a pandas DataFrame\n",
    "                try:\n",
    "                    df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "                    document_structure['elements'].append({\n",
    "                        'content': df,\n",
    "                        'type': 'table',\n",
    "                        'pandas_table': True\n",
    "                    })\n",
    "                except:\n",
    "                    # Fallback to string representation\n",
    "                    document_structure['elements'].append({\n",
    "                        'content': str(table_data),\n",
    "                        'type': 'table',\n",
    "                        'pandas_table': False\n",
    "                    })\n",
    "\n",
    "        # Extract images\n",
    "        for img in soup.find_all('img'):\n",
    "            document_structure['elements'].append({\n",
    "                'content': img.get('alt', 'Image') + f\" (src: {img.get('src', '')})\",\n",
    "                'type': 'image',\n",
    "            })\n",
    "\n",
    "        return document_structure\n",
    "\n",
    "    def _process_text(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process plain text or markdown documents.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        document_structure = {\n",
    "            'metadata': {'source': file_path, 'type': 'text'},\n",
    "            'elements': []\n",
    "        }\n",
    "\n",
    "        # Split by double newlines to separate paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "\n",
    "            # Check if it's a heading (starts with # in markdown)\n",
    "            if para.startswith('#'):\n",
    "                level = len(re.match(r'^#+', para).group())\n",
    "                heading_text = para.lstrip('#').strip()\n",
    "                document_structure['elements'].append({\n",
    "                    'content': heading_text,\n",
    "                    'type': 'heading',\n",
    "                    'level': level\n",
    "                })\n",
    "            # Check if it's a table (simple detection for markdown tables)\n",
    "            elif '|' in para and '-+-' in para.replace('|', '+'):\n",
    "                document_structure['elements'].append({\n",
    "                    'content': para,\n",
    "                    'type': 'table',\n",
    "                    'pandas_table': False\n",
    "                })\n",
    "            # Check for strikeout text (~~text~~ in markdown)\n",
    "            elif '~~' in para:\n",
    "                document_structure['elements'].append({\n",
    "                    'content': para,\n",
    "                    'type': 'strikeout',\n",
    "                })\n",
    "            # Check for highlighted text (** or __ in markdown)\n",
    "            elif '**' in para or '__' in para:\n",
    "                document_structure['elements'].append({\n",
    "                    'content': para,\n",
    "                    'type': 'highlight',\n",
    "                })\n",
    "            # Regular paragraph\n",
    "            else:\n",
    "                document_structure['elements'].append({\n",
    "                    'content': para,\n",
    "                    'type': 'paragraph',\n",
    "                })\n",
    "\n",
    "        return document_structure\n",
    "\n",
    "    def _process_word(self, file_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process Word documents using python-docx library.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the Word document\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with structured document content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import docx\n",
    "            from docx.oxml.table import CT_Tbl\n",
    "            from docx.oxml.text.paragraph import CT_P\n",
    "            \n",
    "            document = docx.Document(file_path)\n",
    "            document_structure = {\n",
    "                'metadata': {'source': file_path, 'type': 'docx'},\n",
    "                'elements': []\n",
    "            }\n",
    "            \n",
    "            # Process document body\n",
    "            for element in document.element.body:\n",
    "                # Process paragraphs\n",
    "                if isinstance(element, CT_P):\n",
    "                    paragraph = docx.Document().add_paragraph()\n",
    "                    paragraph._p = element\n",
    "                    text = paragraph.text.strip()\n",
    "                    \n",
    "                    if not text:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if it's likely a heading (based on style)\n",
    "                    p = document.add_paragraph()\n",
    "                    p._p = element\n",
    "                    if hasattr(p, 'style') and p.style and 'heading' in p.style.name.lower():\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': text,\n",
    "                            'type': 'heading',\n",
    "                            'level': int(p.style.name[-1]) if p.style.name[-1].isdigit() else 1\n",
    "                        })\n",
    "                    # Check for highlighted or strikeout text\n",
    "                    elif '**' in text or '__' in text:\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': text,\n",
    "                            'type': 'highlight'\n",
    "                        })\n",
    "                    elif '~~' in text:\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': text,\n",
    "                            'type': 'strikeout'\n",
    "                        })\n",
    "                    else:\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': text,\n",
    "                            'type': 'paragraph'\n",
    "                        })\n",
    "                \n",
    "                # Process tables\n",
    "                elif isinstance(element, CT_Tbl):\n",
    "                    table = docx.Document().add_table(rows=1, cols=1)\n",
    "                    table._tbl = element\n",
    "                    \n",
    "                    # Convert table to pandas DataFrame\n",
    "                    data = []\n",
    "                    headers = []\n",
    "                    \n",
    "                    # Get headers from first row\n",
    "                    if table.rows:\n",
    "                        for cell in table.rows[0].cells:\n",
    "                            headers.append(cell.text.strip())\n",
    "                    \n",
    "                    # Get data from remaining rows\n",
    "                    for row in table.rows[1:]:\n",
    "                        row_data = []\n",
    "                        for cell in row.cells:\n",
    "                            row_data.append(cell.text.strip())\n",
    "                        data.append(row_data)\n",
    "                    \n",
    "                    # Create pandas DataFrame if possible\n",
    "                    try:\n",
    "                        if headers and data:\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                            document_structure['elements'].append({\n",
    "                                'content': df,\n",
    "                                'type': 'table',\n",
    "                                'pandas_table': True\n",
    "                            })\n",
    "                        else:\n",
    "                            # Create simple text representation for table\n",
    "                            table_text = \"Table content:\\n\"\n",
    "                            for row in table.rows:\n",
    "                                row_text = [cell.text.strip() for cell in row.cells]\n",
    "                                table_text += \" | \".join(row_text) + \"\\n\"\n",
    "                            document_structure['elements'].append({\n",
    "                                'content': table_text,\n",
    "                                'type': 'table',\n",
    "                                'pandas_table': False\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting table to DataFrame: {e}\")\n",
    "                        table_text = \"Table content (error converting):\\n\"\n",
    "                        for row in table.rows:\n",
    "                            row_text = [cell.text.strip() for cell in row.cells]\n",
    "                            table_text += \" | \".join(row_text) + \"\\n\"\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': table_text,\n",
    "                            'type': 'table',\n",
    "                            'pandas_table': False\n",
    "                        })\n",
    "            \n",
    "            # Process document properties (metadata)\n",
    "            try:\n",
    "                core_properties = document.core_properties\n",
    "                document_structure['metadata']['title'] = core_properties.title\n",
    "                document_structure['metadata']['author'] = core_properties.author\n",
    "                document_structure['metadata']['created'] = str(core_properties.created)\n",
    "                document_structure['metadata']['modified'] = str(core_properties.modified)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting document properties: {e}\")\n",
    "            \n",
    "            return document_structure\n",
    "        \n",
    "        except ImportError:\n",
    "            print(\"python-docx package not found. Please install it with 'pip install python-docx'\")\n",
    "            return {\n",
    "                'metadata': {'source': file_path, 'type': 'docx'},\n",
    "                'elements': [{\n",
    "                    'content': f\"python-docx package required for Word processing: {file_path}\",\n",
    "                    'type': 'paragraph',\n",
    "                }]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Word document: {e}\")\n",
    "            return {\n",
    "                'metadata': {'source': file_path, 'type': 'docx'},\n",
    "                'elements': [{\n",
    "                    'content': f\"Error processing Word document: {file_path}. Error: {str(e)}\",\n",
    "                    'type': 'paragraph',\n",
    "                }]\n",
    "            }\n",
    "\n",
    "    def extract_tables_as_triplets(self, document_structure: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract tables from document and convert to triplets for Neo4j.\n",
    "        \"\"\"\n",
    "        triplets = []\n",
    "        table_count = 0\n",
    "        \n",
    "        try:\n",
    "            for idx, element in enumerate(document_structure['elements']):\n",
    "                if element['type'] == 'table':\n",
    "                    if element.get('pandas_table', False):\n",
    "                        try:\n",
    "                            df = element['content']\n",
    "                            if isinstance(df, pd.DataFrame):\n",
    "                                # Get table name\n",
    "                                table_name = f\"Table_{table_count}\"\n",
    "                                table_count += 1\n",
    "                                \n",
    "                                # Look for the closest heading before this table\n",
    "                                for j in range(idx-1, -1, -1):\n",
    "                                    if j < len(document_structure['elements']) and document_structure['elements'][j]['type'] == 'heading':\n",
    "                                        table_name = document_structure['elements'][j]['content']\n",
    "                                        break\n",
    "                                        \n",
    "                                # Create triplets from table\n",
    "                                row_count = 0\n",
    "                                for _, row in df.iterrows():\n",
    "                                    for col in df.columns:\n",
    "                                        try:\n",
    "                                            triplets.append({\n",
    "                                                'subject': f\"{table_name}\",\n",
    "                                                'predicate': str(col),\n",
    "                                                'object': str(row[col]),\n",
    "                                                'row_id': row_count\n",
    "                                            })\n",
    "                                        except Exception as e:\n",
    "                                            print(f\"Error creating triplet for cell: {e}\")\n",
    "                                    row_count += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing table at index {idx}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_tables_as_triplets: {e}\")\n",
    "            \n",
    "        return triplets\n",
    "\n",
    "\n",
    "# ## 3. Implementing Chunking Strategies\n",
    "# Now we'll develop functions for various chunking strategies:\n",
    "# 1. Title + Content (Hierarchical) chunking\n",
    "# 2. Paragraph-Based chunking\n",
    "# 3. Table-Aware chunking for Neo4j Graph representation\n",
    "# 4. Metadata-Aware chunking\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"Class implementing various chunking strategies for document processing.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize text splitters for different strategies\n",
    "        self.recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400,\n",
    "            chunk_overlap=100,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        self.md_header_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=[\n",
    "                (\"#\", \"header1\"),\n",
    "                (\"##\", \"header2\"),\n",
    "                (\"###\", \"header3\"),\n",
    "                (\"####\", \"header4\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.html_header_splitter = HTMLHeaderTextSplitter(\n",
    "            headers_to_split_on=[\n",
    "                (\"h1\", \"header1\"),\n",
    "                (\"h2\", \"header2\"),\n",
    "                (\"h3\", \"header3\"),\n",
    "                (\"h4\", \"header4\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def hierarchical_chunking(self, document_structure: Dict) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform Title + Content (Hierarchical) chunking.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        # First convert the document structure to a format suitable for hierarchical processing\n",
    "        markdown_content = self._convert_to_markdown_with_headers(document_structure)\n",
    "\n",
    "        # Split using the markdown header splitter\n",
    "        md_header_splits = self.md_header_splitter.split_text(markdown_content)\n",
    "\n",
    "        # Further split long content chunks if necessary\n",
    "        final_chunks = []\n",
    "        for doc in md_header_splits:\n",
    "            # If content is too long, split further\n",
    "            content = doc.page_content\n",
    "            if len(content) > 1000:  # adjust threshold as needed\n",
    "                sub_chunks = self.recursive_splitter.split_text(content)\n",
    "                # Copy metadata to each sub-chunk\n",
    "                for chunk in sub_chunks:\n",
    "                    final_chunks.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={**doc.metadata, 'chunk_type': 'hierarchical'}\n",
    "                    ))\n",
    "            else:\n",
    "                doc.metadata['chunk_type'] = 'hierarchical'\n",
    "                final_chunks.append(doc)\n",
    "\n",
    "        return final_chunks\n",
    "\n",
    "    def paragraph_based_chunking(self, document_structure: Dict) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform Paragraph-Based chunking with line breaks preserved.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "\n",
    "        # Extract paragraphs from the document\n",
    "        paragraphs = []\n",
    "        current_section = {'title': 'Document', 'content': ''}\n",
    "\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'heading':\n",
    "                # If we have content in the current section, save it\n",
    "                if current_section['content'].strip():\n",
    "                    paragraphs.append(current_section)\n",
    "\n",
    "                # Start a new section\n",
    "                current_section = {\n",
    "                    'title': element['content'],\n",
    "                    'content': ''\n",
    "                }\n",
    "            elif element['type'] == 'paragraph':\n",
    "                current_section['content'] += element['content'] + \"\\n\\n\"\n",
    "\n",
    "        # Add the last section if it has content\n",
    "        if current_section['content'].strip():\n",
    "            paragraphs.append(current_section)\n",
    "\n",
    "        # Create Document objects for each paragraph\n",
    "        for para in paragraphs:\n",
    "            # Split content if it's too long\n",
    "            if len(para['content']) > 1000:\n",
    "                sub_chunks = self.recursive_splitter.split_text(para['content'])\n",
    "                for i, chunk in enumerate(sub_chunks):\n",
    "                    chunks.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            'title': para['title'],\n",
    "                            'chunk_type': 'paragraph',\n",
    "                            'chunk_index': i,\n",
    "                            'source': document_structure['metadata']['source']\n",
    "                        }\n",
    "                    ))\n",
    "            else:\n",
    "                chunks.append(Document(\n",
    "                    page_content=para['content'],\n",
    "                    metadata={\n",
    "                        'title': para['title'],\n",
    "                        'chunk_type': 'paragraph',\n",
    "                        'source': document_structure['metadata']['source']\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def table_aware_chunking(self, document_structure: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform Table-Aware chunking for Neo4j graph.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries representing table triplets for Neo4j\n",
    "        \"\"\"\n",
    "        # Extract tables and convert to triplets\n",
    "        analyzer = DocumentAnalyzer()\n",
    "        return analyzer.extract_tables_as_triplets(document_structure)\n",
    "\n",
    "    def metadata_aware_chunking(self, document_structure: Dict) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform Metadata-Aware chunking with Milvus compatibility.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Identify semantic sections\n",
    "        semantic_sections = self._identify_semantic_sections(document_structure)\n",
    "        \n",
    "        for section in semantic_sections:\n",
    "            # Calculate chunk size based on content type\n",
    "            if section['type'] == 'rule_explanation':\n",
    "                chunk_size = 1500\n",
    "            elif section['type'] == 'user_scenario':\n",
    "                chunk_size = 1000\n",
    "            elif section['type'] == 'business_logic':\n",
    "                chunk_size = 800\n",
    "            else:\n",
    "                chunk_size = 1000\n",
    "                \n",
    "            custom_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=min(100, chunk_size // 10),\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            # Split content\n",
    "            text_chunks = custom_splitter.split_text(section['content'])\n",
    "            \n",
    "            # Convert keywords and categories to strings for Milvus compatibility\n",
    "            keywords_str = \",\".join(section.get('keywords', []))\n",
    "            categories_str = \",\".join(section.get('categories', []))\n",
    "            \n",
    "            # Create Document objects with compatible metadata\n",
    "            for i, chunk in enumerate(text_chunks):\n",
    "                chunks.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        'title': section.get('title', 'Untitled Section'),\n",
    "                        'content_type': section['type'],\n",
    "                        'source': document_structure['metadata']['source'],\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(text_chunks),\n",
    "                        'chunk_type': 'metadata_aware',\n",
    "                        # Store keywords and categories as strings\n",
    "                        'keywords_str': keywords_str,\n",
    "                        'categories_str': categories_str\n",
    "                    }\n",
    "                ))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _convert_to_markdown_with_headers(self, document_structure: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Convert document structure to markdown format with headers.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            Markdown string representation of the document\n",
    "        \"\"\"\n",
    "        markdown = []\n",
    "        current_heading = \"\"\n",
    "\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'heading':\n",
    "                level = element.get('level', 1)\n",
    "                heading = '#' * level + ' ' + element['content']\n",
    "                markdown.append(heading)\n",
    "                current_heading = element['content']\n",
    "            elif element['type'] == 'paragraph':\n",
    "                markdown.append(element['content'])\n",
    "            elif element['type'] == 'table':\n",
    "                if isinstance(element['content'], pd.DataFrame):\n",
    "                    table_str = element['content'].to_markdown()\n",
    "                    markdown.append(table_str)\n",
    "                else:\n",
    "                    markdown.append(str(element['content']))\n",
    "            elif element['type'] in ['strikeout', 'highlight', 'image']:\n",
    "                markdown.append(element['content'])\n",
    "\n",
    "        return '\\n\\n'.join(markdown)\n",
    "\n",
    "    def _identify_semantic_sections(self, document_structure: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Identify semantic sections in the document.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of semantic section dictionaries\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        current_section = None\n",
    "\n",
    "        # Patterns to identify content types\n",
    "        rule_patterns = ['rule', 'regulation', 'policy', 'guidelines']\n",
    "        logic_patterns = ['workflow', 'process', 'logic', 'procedure', 'business logic']\n",
    "        scenario_patterns = ['scenario', 'example', 'use case', 'user story']\n",
    "        interaction_patterns = ['interaction', 'interface', 'system', 'integration', 'api']\n",
    "\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'heading':\n",
    "                # If there's a current section with content, save it\n",
    "                if current_section and current_section.get('content'):\n",
    "                    sections.append(current_section)\n",
    "\n",
    "                # Determine the section type based on heading content\n",
    "                heading_lower = element['content'].lower()\n",
    "\n",
    "                if any(pattern in heading_lower for pattern in rule_patterns):\n",
    "                    section_type = 'rule_explanation'\n",
    "                elif any(pattern in heading_lower for pattern in logic_patterns):\n",
    "                    section_type = 'business_logic'\n",
    "                elif any(pattern in heading_lower for pattern in scenario_patterns):\n",
    "                    section_type = 'user_scenario'\n",
    "                elif any(pattern in heading_lower for pattern in interaction_patterns):\n",
    "                    section_type = 'system_interaction'\n",
    "                else:\n",
    "                    section_type = 'general'\n",
    "\n",
    "                # Create a new section\n",
    "                current_section = {\n",
    "                    'title': element['content'],\n",
    "                    'type': section_type,\n",
    "                    'content': '',\n",
    "                    'keywords': self._extract_keywords(element['content']),\n",
    "                    'categories': [section_type]\n",
    "                }\n",
    "            elif element['type'] in ['paragraph', 'strikeout', 'highlight']:\n",
    "                if current_section:\n",
    "                    current_section['content'] += element['content'] + \"\\n\\n\"\n",
    "                else:\n",
    "                    # Create a default section if none exists\n",
    "                    current_section = {\n",
    "                        'title': 'Introduction',\n",
    "                        'type': 'general',\n",
    "                        'content': element['content'] + \"\\n\\n\",\n",
    "                        'keywords': [],\n",
    "                        'categories': ['general']\n",
    "                    }\n",
    "            elif element['type'] == 'table':\n",
    "                # Tables are handled separately for Neo4j, but we still include their text representation\n",
    "                # in the content for completeness\n",
    "                if current_section:\n",
    "                    if isinstance(element['content'], pd.DataFrame):\n",
    "                        current_section['content'] += \"Table content:\\n\"\n",
    "                        current_section['content'] += str(element['content']) + \"\\n\\n\"\n",
    "                    else:\n",
    "                        current_section['content'] += \"Table content:\\n\" + str(element['content']) + \"\\n\\n\"\n",
    "\n",
    "        # Add the last section if it exists\n",
    "        if current_section and current_section.get('content'):\n",
    "            sections.append(current_section)\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def _extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract keywords from text.\n",
    "\n",
    "        Args:\n",
    "            text: Text to extract keywords from\n",
    "\n",
    "        Returns:\n",
    "            List of keywords\n",
    "        \"\"\"\n",
    "        # Simple implementation - extract meaningful words and filter stop words\n",
    "        stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'if', 'then', 'else', 'when', \n",
    "                     'on', 'in', 'at', 'to', 'for', 'with', 'by', 'about', 'as', 'of'}\n",
    "\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "        keywords = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Return unique keywords\n",
    "        return list(set(keywords))\n",
    "\n",
    "\n",
    "# ## 4. Setting Up the Embedding Model\n",
    "# We'll configure a local embedding model to generate vector representations of our chunks.\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Class to manage embeddings for document chunks.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the HuggingFace model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        print(f\"Initialized embedding model: {model_name}\")\n",
    "\n",
    "    def generate_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate an embedding for text.\n",
    "\n",
    "        Args:\n",
    "            text: Text to embed\n",
    "\n",
    "        Returns:\n",
    "            List of floats representing the embedding vector\n",
    "        \"\"\"\n",
    "        # The HuggingFaceEmbeddings class follows the LangChain embedding interface\n",
    "        return self.embeddings.embed_query(text)\n",
    "\n",
    "    def batch_embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Batch embed multiple texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of texts to embed\n",
    "\n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        return self.embeddings.embed_documents(texts)\n",
    "\n",
    "    def embed_documents(self, documents: List[Document]) -> Tuple[List[List[float]], List[Document]]:\n",
    "        \"\"\"\n",
    "        Embed LangChain Document objects.\n",
    "\n",
    "        Args:\n",
    "            documents: List of Document objects to embed\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (embeddings, documents)\n",
    "        \"\"\"\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.batch_embed(texts)\n",
    "        return embeddings, documents\n",
    "\n",
    "    def embed_triplets(self, triplets: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Embed triplets for Neo4j graph.\n",
    "\n",
    "        Args:\n",
    "            triplets: List of triplet dictionaries\n",
    "\n",
    "        Returns:\n",
    "            Triplets with embeddings added\n",
    "        \"\"\"\n",
    "        for triplet in triplets:\n",
    "            # Create a concatenated text representation of the triplet\n",
    "            triplet_text = f\"{triplet['subject']} {triplet['predicate']} {triplet['object']}\"\n",
    "            triplet['embedding'] = self.generate_embedding(triplet_text)\n",
    "\n",
    "        return triplets\n",
    "\n",
    "\n",
    "# ## 5. Configuring Vector Databases (Milvus and Neo4j)\n",
    "# Let's set up connections and schemas for Milvus and Neo4j databases.\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "class VectorDBManager:\n",
    "    \"\"\"Class to manage vector database connections and operations.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the vector database manager.\n",
    "\n",
    "        Args:\n",
    "            embedding_manager: Embedding manager to use for generating embeddings\n",
    "        \"\"\"\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.milvus_client = None\n",
    "        self.neo4j_client = None\n",
    "\n",
    "        # Collections in Milvus\n",
    "        self.collections = {\n",
    "            'rule_explanation': None,\n",
    "            'business_logic': None,\n",
    "            'user_scenario': None,\n",
    "            'system_interaction': None\n",
    "        }\n",
    "\n",
    "    def setup_milvus(self, host='localhost', port='2379'):\n",
    "        \"\"\"\n",
    "        Set up connection to Milvus.\n",
    "        \n",
    "        Args:\n",
    "            host: Milvus host\n",
    "            port: Milvus port\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Define Milvus connection parameters\n",
    "            self.milvus_connection_params = {\n",
    "                \"host\": host,\n",
    "                \"port\": port\n",
    "            }\n",
    "            \n",
    "            print(\"Milvus connection parameters configured. Collections will be created when data is inserted.\")\n",
    "            self.milvus_available = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up Milvus: {e}\")\n",
    "            self.milvus_available = False\n",
    "\n",
    "    def setup_neo4j(self, uri='bolt://localhost:7687', username='neo4j', password='venev'):\n",
    "        \"\"\"\n",
    "        Set up connection to Neo4j.\n",
    "        \n",
    "        Args:\n",
    "            uri: Neo4j URI\n",
    "            username: Neo4j username\n",
    "            password: Neo4j password\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Store Neo4j connection parameters\n",
    "            self.neo4j_connection_params = {\n",
    "                \"url\": uri,\n",
    "                \"username\": username,\n",
    "                \"password\": password\n",
    "            }\n",
    "            \n",
    "            # Initialize Neo4j graph client from LangChain\n",
    "            self.neo4j_client = Neo4jGraph(\n",
    "                url=uri,\n",
    "                username=username,\n",
    "                password=password\n",
    "            )\n",
    "            \n",
    "            # Initialize Neo4j vector client with correct parameters\n",
    "            self.neo4j_vector = Neo4jVector(\n",
    "                url=uri,\n",
    "                username=username,\n",
    "                password=password,\n",
    "                embedding=self.embedding_manager.embeddings,\n",
    "                index_name=\"documentVectors\",\n",
    "                node_label=\"DocumentChunk\"\n",
    "            )\n",
    "            \n",
    "            # Set up schema for our document graph\n",
    "            self._create_neo4j_schema()\n",
    "            print(\"Neo4j connection established and schema created.\")\n",
    "            self.neo4j_available = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up Neo4j: {e}\")\n",
    "            self.neo4j_client = None\n",
    "            self.neo4j_vector = None\n",
    "            self.neo4j_available = False\n",
    "\n",
    "    def _create_neo4j_schema(self):\n",
    "        \"\"\"Create Neo4j schema for document storage.\"\"\"\n",
    "        # Create constraints that work with Community Edition\n",
    "        self.neo4j_client.query(\"\"\"\n",
    "            CREATE CONSTRAINT document_id IF NOT EXISTS\n",
    "            FOR (d:Document) REQUIRE d.id IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        self.neo4j_client.query(\"\"\"\n",
    "            CREATE CONSTRAINT chunk_id IF NOT EXISTS\n",
    "            FOR (c:DocumentChunk) REQUIRE c.id IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        # Remove the existence constraint that requires Enterprise Edition\n",
    "        # self.neo4j_client.query(\"\"\"\n",
    "        #    CREATE CONSTRAINT documentchunk_text IF NOT EXISTS\n",
    "        #    FOR (c:DocumentChunk) REQUIRE c.text IS NOT NULL\n",
    "        # \"\"\")\n",
    "        \n",
    "        # Create vector index if needed\n",
    "        self.neo4j_client.query(\"\"\"\n",
    "            CREATE VECTOR INDEX documentVectors IF NOT EXISTS\n",
    "            FOR (c:DocumentChunk) \n",
    "            ON c.embedding\n",
    "            OPTIONS {indexConfig: {\n",
    "              `vector.dimensions`: 384,\n",
    "              `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "        \"\"\")\n",
    "\n",
    "    def store_in_milvus(self, documents: List[Document], content_type: str):\n",
    "        \"\"\"\n",
    "        Store documents in Milvus.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Document objects to store\n",
    "            content_type: Type of content (rule_explanation, business_logic, etc.)\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'milvus_available') or not self.milvus_available:\n",
    "            print(f\"Skipping Milvus storage for {content_type} - Milvus not available\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Check if content type is valid\n",
    "            if content_type not in self.collections:\n",
    "                raise ValueError(f\"Invalid content type: {content_type}\")\n",
    "\n",
    "            # Create collection if it doesn't exist\n",
    "            collection_name = f\"docs_{content_type}\"\n",
    "\n",
    "            # Generate embeddings for documents\n",
    "            embeddings = [self.embedding_manager.generate_embedding(doc.page_content) for doc in documents]\n",
    "\n",
    "            # Create Milvus collection\n",
    "            vector_store = Milvus.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=self.embedding_manager.embeddings,\n",
    "                collection_name=collection_name,\n",
    "                connection_args=self.milvus_connection_params\n",
    "            )\n",
    "\n",
    "            # Store the collection reference\n",
    "            self.collections[content_type] = vector_store\n",
    "\n",
    "            print(f\"Stored {len(documents)} documents in Milvus collection: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing documents in Milvus: {e}\")\n",
    "\n",
    "    def store_in_neo4j(self, triplets: List[Dict]):\n",
    "        \"\"\"\n",
    "        Store triplets in Neo4j.\n",
    "        \n",
    "        Args:\n",
    "            triplets: List of triplet dictionaries to store\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'neo4j_available') or not self.neo4j_available:\n",
    "            print(\"Skipping Neo4j storage - Neo4j not available\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Get the Neo4j driver session properly\n",
    "            # The neo4j_client has a session() method on its driver\n",
    "            session = self.neo4j_client._driver.session()\n",
    "            \n",
    "            # Process in smaller batches for better error handling\n",
    "            batch_size = 50\n",
    "            for i in range(0, len(triplets), batch_size):\n",
    "                batch = triplets[i:i+batch_size]\n",
    "                try:\n",
    "                    # Begin a transaction for this batch\n",
    "                    with session.begin_transaction() as tx:\n",
    "                        # Convert parameters for Neo4j\n",
    "                        params = {\n",
    "                            \"batch_size\": len(batch),\n",
    "                            \"subjects\": [t[\"subject\"] for t in batch],\n",
    "                            \"predicates\": [t[\"predicate\"] for t in batch],\n",
    "                            \"objects\": [t[\"object\"] for t in batch],\n",
    "                            \"embeddings\": [t[\"embedding\"] for t in batch]\n",
    "                        }\n",
    "\n",
    "                        # Fixed Cypher query\n",
    "                        query = \"\"\"\n",
    "                        UNWIND range(0, $batch_size - 1) as i\n",
    "                        MERGE (s:Subject {name: $subjects[i]})\n",
    "                        MERGE (o:Object {name: $objects[i]})\n",
    "                        WITH s, o, i\n",
    "                        CREATE (s)-[r:HAS_PROPERTY {name: $predicates[i]}]->(o)\n",
    "                        WITH s, o, i\n",
    "                        CREATE (c:DocumentChunk {\n",
    "                            id: randomUUID(),\n",
    "                            text: $subjects[i] + ' ' + $predicates[i] + ' ' + $objects[i],\n",
    "                            subject: $subjects[i],\n",
    "                            predicate: $predicates[i],\n",
    "                            object: $objects[i]\n",
    "                        })\n",
    "                        SET c.embedding = $embeddings[i]\n",
    "                        CREATE (c)-[:SUBJECT_OF]->(s)\n",
    "                        CREATE (c)-[:OBJECT_OF]->(o)\n",
    "                        \"\"\"\n",
    "\n",
    "                        # Execute query within transaction\n",
    "                        result = tx.run(query, params)\n",
    "                        result.consume()  # Ensure execution completes\n",
    "                    \n",
    "                    print(f\"Processed batch {i//batch_size + 1}/{(len(triplets) + batch_size - 1)//batch_size}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "                    # Continue to the next batch even if this one fails\n",
    "            \n",
    "            # Close the session when done\n",
    "            session.close()\n",
    "            print(f\"Stored triplets in Neo4j (completed batches)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Neo4j transaction: {e}\")\n",
    "            # Make sure session is closed if an error occurs\n",
    "            if 'session' in locals():\n",
    "                session.close()\n",
    "\n",
    "    def hybrid_retrieval(self, query: str, content_types: List[str], k: int = 5):\n",
    "        \"\"\"\n",
    "        Perform hybrid retrieval from Milvus and Neo4j.\n",
    "\n",
    "        Args:\n",
    "            query: Query string\n",
    "            content_types: List of content types to search\n",
    "            k: Number of results to return from each source\n",
    "\n",
    "        Returns:\n",
    "            List of retrieved documents\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Retrieve from Milvus\n",
    "        for content_type in content_types:\n",
    "            if content_type in self.collections and self.collections[content_type] is not None:\n",
    "                milvus_results = self.collections[content_type].similarity_search(query, k=k)\n",
    "                results.extend(milvus_results)\n",
    "\n",
    "        # Retrieve from Neo4j if available\n",
    "        if self.neo4j_vector:\n",
    "            neo4j_results = self.neo4j_vector.similarity_search(query, k=k)\n",
    "            results.extend(neo4j_results)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# ## 6. Document Processing Pipeline\n",
    "# Now let's develop a pipeline that processes documents by content type, applies appropriate chunking strategies, and stores data in the correct database.\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "class DocumentProcessingPipeline:\n",
    "    \"\"\"Pipeline for document processing, chunking, and storage.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        analyzer: DocumentAnalyzer, \n",
    "        chunker: DocumentChunker, \n",
    "        embedding_manager: EmbeddingManager,\n",
    "        vector_db_manager: VectorDBManager\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the document processing pipeline.\n",
    "\n",
    "        Args:\n",
    "            analyzer: Document analyzer\n",
    "            chunker: Document chunker\n",
    "            embedding_manager: Embedding manager\n",
    "            vector_db_manager: Vector database manager\n",
    "        \"\"\"\n",
    "        self.analyzer = analyzer\n",
    "        self.chunker = chunker\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.vector_db_manager = vector_db_manager\n",
    "\n",
    "        # Content type mapping\n",
    "        self.content_mapping = {\n",
    "            'rule_explanation': {'chunking': 'hierarchical', 'storage': 'milvus'},\n",
    "            'business_logic': {'chunking': 'paragraph', 'storage': 'milvus'},\n",
    "            'table': {'chunking': 'table', 'storage': 'neo4j'},\n",
    "            'user_scenario': {'chunking': 'paragraph', 'storage': 'milvus'},\n",
    "            'system_interaction': {'chunking': 'metadata', 'storage': 'both'}\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Process a document file through the entire pipeline.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "        \"\"\"\n",
    "        print(f\"Processing document: {file_path}\")\n",
    "\n",
    "        # Step 1: Analyze document\n",
    "        document_structure = self.analyzer.load_document(file_path)\n",
    "        print(f\"Document analysis complete. Found {len(document_structure['elements'])} elements.\")\n",
    "\n",
    "        # Step 2: Process each content type with appropriate chunking\n",
    "        content_types = self._identify_content_types(document_structure)\n",
    "        print(f\"Identified content types: {', '.join(content_types)}\")\n",
    "\n",
    "        # Step 3: Apply chunking strategies and store in appropriate databases\n",
    "        self._process_content_types(document_structure, content_types)\n",
    "\n",
    "        print(\"Document processing complete.\")\n",
    "\n",
    "    def _identify_content_types(self, document_structure: Dict) -> List[str]:\n",
    "        \"\"\"\n",
    "        Identify content types in the document structure.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of identified content types\n",
    "        \"\"\"\n",
    "        content_types = set()\n",
    "\n",
    "        # Look for tables explicitly\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'table':\n",
    "                content_types.add('table')\n",
    "\n",
    "        # For other content types, we need more sophisticated analysis\n",
    "        # Convert document to text for analysis\n",
    "        document_text = \"\"\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] in ['heading', 'paragraph']:\n",
    "                document_text += element['content'] + \"\\n\\n\"\n",
    "\n",
    "        # Check for rule explanations\n",
    "        if any(keyword in document_text.lower() for keyword in \n",
    "               ['rule', 'regulation', 'policy', 'guidelines', 'requirement']):\n",
    "            content_types.add('rule_explanation')\n",
    "\n",
    "        # Check for business logic\n",
    "        if any(keyword in document_text.lower() for keyword in \n",
    "               ['workflow', 'process', 'logic', 'procedure', 'if then', 'business logic']):\n",
    "            content_types.add('business_logic')\n",
    "\n",
    "        # Check for user scenarios\n",
    "        if any(keyword in document_text.lower() for keyword in \n",
    "               ['scenario', 'example', 'use case', 'user story', 'user journey']):\n",
    "            content_types.add('user_scenario')\n",
    "\n",
    "        # Check for system interactions\n",
    "        if any(keyword in document_text.lower() for keyword in \n",
    "               ['interaction', 'interface', 'system', 'integration', 'api', 'endpoint']):\n",
    "            content_types.add('system_interaction')\n",
    "\n",
    "        # If no specific types identified, default to business_logic\n",
    "        if not content_types or (content_types == {'table'}):\n",
    "            content_types.add('business_logic')\n",
    "\n",
    "        return list(content_types)\n",
    "\n",
    "    def _process_content_types(self, document_structure: Dict, content_types: List[str]):\n",
    "        \"\"\"Process content types with improved error handling.\"\"\"\n",
    "        # Create a copy of content_types to avoid modification during iteration\n",
    "        types_to_process = content_types.copy()\n",
    "        \n",
    "        # Process tables first if present\n",
    "        if 'table' in types_to_process:\n",
    "            print(\"Processing tables...\")\n",
    "            try:\n",
    "                triplets = self.chunker.table_aware_chunking(document_structure)\n",
    "                \n",
    "                if triplets:\n",
    "                    # Process triplets as before...\n",
    "                    if hasattr(self.vector_db_manager, 'neo4j_available') and self.vector_db_manager.neo4j_available:\n",
    "                        try:\n",
    "                            triplets_with_embeddings = self.embedding_manager.embed_triplets(triplets)\n",
    "                            self.vector_db_manager.store_in_neo4j(triplets_with_embeddings)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error storing triplets in Neo4j: {e}\")\n",
    "                            \n",
    "                    # Fallback to Milvus regardless, for redundancy\n",
    "                    try:\n",
    "                        print(\"Storing table data in Milvus for backup\")\n",
    "                        table_chunks = []\n",
    "                        for triplet in triplets:\n",
    "                            table_chunks.append(Document(\n",
    "                                page_content=f\"{triplet['subject']} - {triplet['predicate']}: {triplet['object']}\",\n",
    "                                metadata={\n",
    "                                    'title': triplet['subject'],\n",
    "                                    'chunk_type': 'table_fallback',\n",
    "                                    'source': document_structure['metadata']['source']\n",
    "                                }\n",
    "                            ))\n",
    "                        if table_chunks:\n",
    "                            self.vector_db_manager.store_in_milvus(table_chunks, 'business_logic')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error storing table fallback in Milvus: {e}\")\n",
    "                else:\n",
    "                    print(\"No valid tables found for extraction\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing tables: {e}\")\n",
    "                \n",
    "            types_to_process.remove('table')\n",
    "        \n",
    "        # Process other content types with better error handling\n",
    "        for content_type in types_to_process:\n",
    "            print(f\"Processing content type: {content_type}\")\n",
    "            \n",
    "            try:\n",
    "                chunking_strategy = self.content_mapping[content_type]['chunking']\n",
    "                storage = self.content_mapping[content_type]['storage']\n",
    "                \n",
    "                # Apply chunking strategy with error handling\n",
    "                chunks = []\n",
    "                try:\n",
    "                    if chunking_strategy == 'hierarchical':\n",
    "                        chunks = self.chunker.hierarchical_chunking(document_structure)\n",
    "                    elif chunking_strategy == 'paragraph':\n",
    "                        chunks = self.chunker.paragraph_based_chunking(document_structure)\n",
    "                    elif chunking_strategy == 'metadata':\n",
    "                        chunks = self.chunker.metadata_aware_chunking(document_structure)\n",
    "                    else:\n",
    "                        print(f\"Unknown chunking strategy for {content_type}: {chunking_strategy}\")\n",
    "                        continue\n",
    "                        \n",
    "                    print(f\"Created {len(chunks)} chunks using {chunking_strategy} strategy\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying chunking strategy '{chunking_strategy}': {e}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Store in appropriate database with fallbacks\n",
    "                if not chunks:\n",
    "                    print(f\"No chunks created for {content_type}. Skipping storage.\")\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # Always try Milvus first for reliability\n",
    "                    if hasattr(self.vector_db_manager, 'milvus_available') and self.vector_db_manager.milvus_available:\n",
    "                        print(f\"Storing {len(chunks)} chunks in Milvus\")\n",
    "                        self.vector_db_manager.store_in_milvus(chunks, content_type)\n",
    "                        \n",
    "                    # If Neo4j storage is required and available\n",
    "                    if (storage == 'neo4j' or storage == 'both'):\n",
    "                        try:\n",
    "                            print(f\"Converting chunks to triplets for Neo4j storage\")\n",
    "                            triplets = self._convert_chunks_to_triplets(chunks, content_type)\n",
    "                            triplets_with_embeddings = self.embedding_manager.embed_triplets(triplets)\n",
    "                            \n",
    "                            # Try Neo4j storage\n",
    "                            if hasattr(self.vector_db_manager, 'neo4j_available') and self.vector_db_manager.neo4j_available:\n",
    "                                try:\n",
    "                                    self.vector_db_manager.store_in_neo4j(triplets_with_embeddings)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error storing in Neo4j: {e}\")\n",
    "                                    print(\"Falling back to Milvus storage for this content\")\n",
    "                                    # Always ensure data is stored somewhere\n",
    "                                    if hasattr(self.vector_db_manager, 'milvus_available') and self.vector_db_manager.milvus_available:\n",
    "                                        triplet_chunks = []\n",
    "                                        for triplet in triplets:\n",
    "                                            triplet_chunks.append(Document(\n",
    "                                                page_content=f\"{triplet['subject']} {triplet['predicate']} {triplet['object']}\",\n",
    "                                                metadata={\n",
    "                                                    'title': f\"{triplet['subject']}\",\n",
    "                                                    'chunk_type': 'neo4j_fallback',\n",
    "                                                    'source': document_structure['metadata']['source'] \n",
    "                                                }\n",
    "                                            ))\n",
    "                                        if triplet_chunks:\n",
    "                                            # Store backup in Milvus\n",
    "                                            self.vector_db_manager.store_in_milvus(triplet_chunks, content_type)\n",
    "                                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error preparing triplets: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error storing chunks for {content_type}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing content type {content_type}: {e}\")\n",
    "\n",
    "    def _convert_chunks_to_triplets(self, chunks: List[Document], content_type: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Convert chunks to triplets for Neo4j storage.\n",
    "\n",
    "        Args:\n",
    "            chunks: List of Document objects\n",
    "            content_type: Type of content\n",
    "\n",
    "        Returns:\n",
    "            List of triplet dictionaries\n",
    "        \"\"\"\n",
    "        triplets = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            # The approach to convert to triplets depends on the content\n",
    "            if content_type == 'system_interaction':\n",
    "                # For system interactions, create subject-action-object triplets\n",
    "                # This is a simplified approach; in practice, you might use an NLP model\n",
    "                title = chunk.metadata.get('title', 'System Interaction')\n",
    "                content = chunk.page_content\n",
    "\n",
    "                # Simple rule-based extraction of subject-verb-object\n",
    "                for sentence in content.split('. '):\n",
    "                    if len(sentence.strip()) < 10:\n",
    "                        continue\n",
    "\n",
    "                    # Very simple SVO extraction (would use proper NLP in production)\n",
    "                    parts = sentence.strip().split(' ', 2)\n",
    "                    if len(parts) >= 3:\n",
    "                        subject = parts[0]\n",
    "                        predicate = parts[1]\n",
    "                        object_val = parts[2]\n",
    "\n",
    "                        triplets.append({\n",
    "                            'subject': f\"{title}_{subject}\",\n",
    "                            'predicate': predicate,\n",
    "                            'object': object_val\n",
    "                        })\n",
    "                    else:\n",
    "                        # Fallback for sentences that don't match our pattern\n",
    "                        triplets.append({\n",
    "                            'subject': title,\n",
    "                            'predicate': 'contains',\n",
    "                            'object': sentence.strip()\n",
    "                        })\n",
    "            else:\n",
    "                # For other content types, use a simpler approach\n",
    "                title = chunk.metadata.get('title', 'Document Section')\n",
    "                content = chunk.page_content\n",
    "\n",
    "                triplets.append({\n",
    "                    'subject': title,\n",
    "                    'predicate': 'contains',\n",
    "                    'object': content\n",
    "                })\n",
    "\n",
    "        return triplets\n",
    "\n",
    "\n",
    "# ## 7. Hybrid Retrieval System with LangChain\n",
    "# Now let's implement a hybrid retrieval system using LangChain v0.3 to combine search results from both Milvus and Neo4j.\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Class for hybrid retrieval from multiple vector databases.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_db_manager: VectorDBManager):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid retriever.\n",
    "\n",
    "        Args:\n",
    "            vector_db_manager: Vector database manager instance\n",
    "        \"\"\"\n",
    "        self.vector_db_manager = vector_db_manager\n",
    "\n",
    "    def setup_langchain_retrieval(self, use_gemini_for_query_expansion=True):\n",
    "        \"\"\"Set up LangChain retrieval with better error handling.\"\"\"\n",
    "        try:\n",
    "            # Create retriever objects for different content types and databases\n",
    "            self.retrievers = {}\n",
    "\n",
    "            # Add Milvus retrievers\n",
    "            for content_type, collection in self.vector_db_manager.collections.items():\n",
    "                if collection is not None:\n",
    "                    try:\n",
    "                        self.retrievers[f\"milvus_{content_type}\"] = collection.as_retriever(\n",
    "                            search_type=\"similarity\",\n",
    "                            search_kwargs={\"k\": 3}\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error setting up retriever for {content_type}: {e}\")\n",
    "\n",
    "            # Add Neo4j retriever if available\n",
    "            if hasattr(self.vector_db_manager, 'neo4j_vector') and self.vector_db_manager.neo4j_vector:\n",
    "                try:\n",
    "                    self.retrievers[\"neo4j\"] = self.vector_db_manager.neo4j_vector.as_retriever(\n",
    "                        search_kwargs={\"k\": 3}\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error setting up Neo4j retriever: {e}\")\n",
    "            \n",
    "            # Set up query expansion if requested\n",
    "            if use_gemini_for_query_expansion:\n",
    "                try:\n",
    "                    if \"GOOGLE_API_KEY\" in os.environ and os.environ[\"GOOGLE_API_KEY\"]:\n",
    "                        # Set up Gemini for multi-query generation\n",
    "                        self.gemini_llm = ChatGoogleGenerativeAI(\n",
    "                            model=\"gemini-1.5-flash-8b\",\n",
    "                            temperature=0.2\n",
    "                        )\n",
    "\n",
    "                        # Set up for main retrievers\n",
    "                        for retriever_name in list(self.retrievers.keys()):\n",
    "                            if retriever_name.startswith(\"milvus_\"):\n",
    "                                try:\n",
    "                                    self.multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "                                        retriever=self.retrievers[retriever_name],\n",
    "                                        llm=self.gemini_llm\n",
    "                                    )\n",
    "                                    print(\"Multi-query retriever set up with Gemini\")\n",
    "                                    break\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error setting up multi-query with {retriever_name}: {e}\")\n",
    "                    else:\n",
    "                        print(\"No Google API key found. Skipping query expansion setup.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error setting up query expansion: {e}\")\n",
    "\n",
    "            print(f\"Set up {len(self.retrievers)} retrievers for hybrid search\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up LangChain retrieval: {e}\")\n",
    "            # Initialize empty retrievers to avoid errors\n",
    "            self.retrievers = {}\n",
    "\n",
    "    def hybrid_retrieve(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Perform hybrid retrieval from all configured retrievers.\n",
    "        \n",
    "        Args:\n",
    "            query: Query string\n",
    "            k: Number of results to return per retriever\n",
    "            \n",
    "        Returns:\n",
    "            Combined list of unique retrieved documents\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        active_retrievers = 0\n",
    "        \n",
    "        # Check if there are any retrievers configured\n",
    "        if not hasattr(self, 'retrievers') or not self.retrievers:\n",
    "            print(\"No retrievers configured. Returning empty results.\")\n",
    "            return []\n",
    "            \n",
    "        # Use multi-query retriever if available\n",
    "        if hasattr(self, 'multi_query_retriever'):\n",
    "            try:\n",
    "                print(\"Using multi-query retriever\")\n",
    "                multi_results = self.multi_query_retriever.invoke(query)\n",
    "                all_results.extend(multi_results)\n",
    "                active_retrievers += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error with multi-query retrieval: {e}\")\n",
    "        \n",
    "        # Use all other retrievers\n",
    "        for name, retriever in self.retrievers.items():\n",
    "            try:\n",
    "                print(f\"Querying retriever: {name}\")\n",
    "                results = retriever.invoke(query)  # Using invoke instead of get_relevant_documents\n",
    "                all_results.extend(results)\n",
    "                active_retrievers += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving from {name}: {e}\")\n",
    "        \n",
    "        if active_retrievers == 0:\n",
    "            print(\"Warning: No retrievers were successfully queried.\")\n",
    "        \n",
    "        # Remove duplicates (simplified - in practice should use better deduplication)\n",
    "        seen_contents = set()\n",
    "        unique_results = []\n",
    "        \n",
    "        for doc in all_results:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_contents:\n",
    "                seen_contents.add(content)\n",
    "                unique_results.append(doc)\n",
    "        \n",
    "        # Return the top k results\n",
    "        return unique_results[:k]\n",
    "\n",
    "    def create_langchain_rag_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create a LangChain RAG pipeline with hybrid retrieval.\n",
    "\n",
    "        Returns:\n",
    "            Runnable chain for RAG\n",
    "        \"\"\"\n",
    "        # Set up Gemini LLM if API key is available\n",
    "        if \"GOOGLE_API_KEY\" in os.environ:\n",
    "            print(\"Creating LangChain RAG pipeline with Gemini\")\n",
    "\n",
    "            llm = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-1.5-flash-8b\",\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                top_k=40,\n",
    "                max_output_tokens=2048,\n",
    "            )\n",
    "\n",
    "            # Create a prompt template\n",
    "            template = \"\"\"\n",
    "            You are an AI assistant that provides accurate and helpful information based on the given context.\n",
    "\n",
    "            Context information:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            If the context doesn't contain relevant information, please say \"I don't have enough information to answer this question.\"\n",
    "            Provide a comprehensive answer based on the context provided:\n",
    "            \"\"\"\n",
    "\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "            # Create a retriever that combines results from all our retrievers\n",
    "            def retrieve_docs(query):\n",
    "                return self.hybrid_retrieve(query, k=5)\n",
    "\n",
    "            # Build the RAG chain\n",
    "            rag_chain = (\n",
    "                {\"context\": retrieve_docs, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            return rag_chain\n",
    "        else:\n",
    "            print(\"Google API key not found, cannot create RAG pipeline\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# ## 8. Query Processing with Gemini LLM\n",
    "# Let's integrate the Gemini-1.5-flash-8b model via API to process queries against the retrieved document chunks.\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "class GeminiProcessor:\n",
    "    \"\"\"Class for processing queries with Gemini LLM.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the Gemini processor.\n",
    "\n",
    "        Args:\n",
    "            api_key: Google API key for Gemini\n",
    "        \"\"\"\n",
    "        # Try to get API key from parameter, then environment variable\n",
    "        self.api_key = api_key or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        \n",
    "        # Remove the streamlit dependency\n",
    "        if not self.api_key:\n",
    "            print(\"Warning: No API key provided for Gemini. Set GOOGLE_API_KEY environment variable.\")\n",
    "        else:\n",
    "            try:\n",
    "                genai.configure(api_key=self.api_key)\n",
    "                self.model = genai.GenerativeModel('gemini-1.5-flash-8b')\n",
    "                print(\"Gemini model initialized successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing Gemini: {e}\")\n",
    "                self.model = None\n",
    "\n",
    "    def process_query(self, query: str, retrieved_docs: List[Document]) -> str:\n",
    "        \"\"\"\n",
    "        Process a query using the Gemini LLM and retrieved documents.\n",
    "\n",
    "        Args:\n",
    "            query: User query\n",
    "            retrieved_docs: List of retrieved Document objects\n",
    "\n",
    "        Returns:\n",
    "            Response from Gemini\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            return \"Gemini model not initialized. Please provide a valid API key.\"\n",
    "\n",
    "        try:\n",
    "            # Prepare context from retrieved documents\n",
    "            context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "            # Format prompt for Gemini\n",
    "            prompt = f\"\"\"\n",
    "            You are an intelligent assistant that answers questions based on provided context.\n",
    "\n",
    "            Context information:\n",
    "            {context}\n",
    "            User question: {query}\n",
    "            Please answer the question based only on the provided context. If the context doesn't contain enough information \n",
    "            to fully answer the question, acknowledge what you can answer based on the context and what information is missing.\n",
    "            \"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            response = self.model.generate_content(prompt)\n",
    "            return response.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query with Gemini: {e}\")\n",
    "            return f\"Error processing query: {str(e)}\"\n",
    "\n",
    "    def generate_query_variations(self, query: str, num_variations: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate variations of a query for better retrieval.\n",
    "\n",
    "        Args:\n",
    "            query: Original query\n",
    "            num_variations: Number of variations to generate\n",
    "\n",
    "        Returns:\n",
    "            List of query variations\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            return [query]  # Return original query if model not available\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Generate {num_variations} different ways to ask the following question. \n",
    "            Return only the list of questions without any explanations or numbering.\n",
    "            Original question: {query}\n",
    "            \"\"\"\n",
    "            response = self.model.generate_content(prompt)\n",
    "            variations = [line.strip() for line in response.text.split('\\n') if line.strip()]\n",
    "            # Include the original query\n",
    "            if query not in variations:\n",
    "                variations.insert(0, query)\n",
    "\n",
    "            # Limit to requested number\n",
    "            return variations[:num_variations]\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating query variations: {e}\")\n",
    "            return [query]  # Return original query on error\n",
    "\n",
    "\n",
    "# ## 9. Demo and Usage Examples\n",
    "# Let's create a complete demonstration of the system functionality.\n",
    "\n",
    "\n",
    "\n",
    "def process_document(file_path: str, milvus_host='localhost', milvus_port='19530', \n",
    "                    neo4j_uri='bolt://localhost:7687', neo4j_username='neo4j', neo4j_password='venev'):\n",
    "    \"\"\"Process a document through the entire pipeline.\"\"\"\n",
    "    # Initialize components\n",
    "    analyzer = DocumentAnalyzer()\n",
    "    chunker = DocumentChunker()\n",
    "    embedding_manager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Initialize vector database manager\n",
    "    vector_db_manager = VectorDBManager(embedding_manager)\n",
    "    \n",
    "    # Connect to databases with provided connection parameters\n",
    "    # Add error handling so one failure doesn't stop everything\n",
    "    try:\n",
    "        vector_db_manager.setup_milvus(host=milvus_host, port=milvus_port)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to set up Milvus: {e}\")\n",
    "    \n",
    "    try:\n",
    "        vector_db_manager.setup_neo4j(uri=neo4j_uri, username=neo4j_username, password=neo4j_password)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to set up Neo4j: {e}\")\n",
    "    \n",
    "    # Create and run the processing pipeline\n",
    "    pipeline = DocumentProcessingPipeline(\n",
    "        analyzer=analyzer,\n",
    "        chunker=chunker, \n",
    "        embedding_manager=embedding_manager,\n",
    "        vector_db_manager=vector_db_manager\n",
    "    )\n",
    "    \n",
    "    # Process the document\n",
    "    try:\n",
    "        pipeline.process_document(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during document processing: {e}\")\n",
    "    \n",
    "    # Set up retrieval system\n",
    "    retriever = HybridRetriever(vector_db_manager)\n",
    "    \n",
    "    try:\n",
    "        retriever.setup_langchain_retrieval(use_gemini_for_query_expansion=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up LangChain retrieval: {e}\")\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "def query_document(retriever, query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Query the document storage system.\n",
    "    \n",
    "    Args:\n",
    "        retriever: HybridRetriever instance from process_document\n",
    "        query: Query string to search for\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (retrieved documents, LLM response if available)\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    documents = retriever.hybrid_retrieve(query, k=k)\n",
    "    \n",
    "    # Process with LLM if API key is available\n",
    "    if \"GOOGLE_API_KEY\" in os.environ:\n",
    "        gemini_processor = GeminiProcessor()\n",
    "        response = gemini_processor.process_query(query, documents)\n",
    "        return documents, response\n",
    "    else:\n",
    "        return documents, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e259fcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_API_KEY=\"AIzaSyD0YvhLFGBmYiS2KYnuRjG1F-xdxJ2pmO4\"\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_API_KEY=\"AIzaSyD0YvhLFGBmYiS2KYnuRjG1F-xdxJ2pmO4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1307a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedding model: all-MiniLM-L6-v2\n",
      "Milvus connection parameters configured. Collections will be created when data is inserted.\n",
      "Neo4j connection established and schema created.\n",
      "Processing document: knowledge-base/data/IT449_HR Roll back for policy Cancellation v2.1.docx\n",
      "Error processing Word document: Package not found at 'knowledge-base/data/IT449_HR Roll back for policy Cancellation v2.1.docx'\n",
      "Document analysis complete. Found 1 elements.\n",
      "Identified content types: rule_explanation, business_logic\n",
      "Processing content type: rule_explanation\n",
      "Created 1 chunks using hierarchical strategy\n",
      "Storing 1 chunks in Milvus\n",
      "Stored 1 documents in Milvus collection: docs_rule_explanation\n",
      "Processing content type: business_logic\n",
      "Created 1 chunks using paragraph strategy\n",
      "Storing 1 chunks in Milvus\n",
      "Stored 1 documents in Milvus collection: docs_business_logic\n",
      "Document processing complete.\n",
      "Multi-query retriever set up with Gemini\n",
      "Set up 3 retrievers for hybrid search\n",
      "Using multi-query retriever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: text)} {position: line: 1, column: 162, offset: 161} for query: 'CALL db.index.vector.queryNodes($vector_index_name, $top_k * $effective_search_ratio, $query_vector) YIELD node, score WITH node, score LIMIT $top_k RETURN node.`text` AS text, score, node {.*, `text`: Null, `embedding`: Null, id: Null } AS metadata'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with multi-query retrieval: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "Querying retriever: milvus_rule_explanation\n",
      "Querying retriever: milvus_business_logic\n",
      "Querying retriever: neo4j\n",
      "Gemini model initialized successfully\n",
      "Error processing query with Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "Retrieved documents: [Document(metadata={'chunk_type': 'hierarchical', 'pk': 457806402026733845}, page_content=\"Error processing Word document: knowledge-base/data/IT449_HR Roll back for policy Cancellation v2.1.docx. Error: Package not found at 'knowledge-base/data/IT449_HR Roll back for policy Cancellation v2.1.docx'\"), Document(metadata={'title': 'Document', 'chunk_type': 'paragraph', 'source': 'knowledge-base/data/IT449_HR Roll back for policy Cancellation v2.1.docx', 'pk': 457806402026733847}, page_content=\"Error processing Word document: knowledge-base/data/IT449_HR Roll back for policy Cancellation v2.1.docx. Error: Package not found at 'knowledge-base/data/IT449_HR Roll back for policy Cancellation v2.1.docx'\\n\\n\")] \n",
      "LLM answer: Error processing query: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "retriever = process_document(\"knowledge-base/data/IT449_HR Roll back for policy Cancellation v2.1.docx\")\n",
    "results, answer = query_document(retriever, \"What should HR system do?\")\n",
    "print(\"Retrieved documents:\", results, \"\\nLLM answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92ad52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
