{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a3f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/fantastic-engine/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Any, Optional, Union\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    HTMLHeaderTextSplitter\n",
    ")\n",
    "from langchain_community.vectorstores.milvus import Milvus\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Document processing\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "import tabula\n",
    "\n",
    "# Google Gemini\n",
    "import google.generativeai as genai\n",
    "from tqdm.auto import tqdm as notebook_tqdm\n",
    "print(\"All required libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009b1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fde16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAnalyzer:\n",
    "    \"\"\"Class to analyze document structure and extract various elements.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_document(self, file_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Load document from file path and return structured content.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with structured document content\n",
    "        \"\"\"\n",
    "        # Get file extension\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "        ext = ext.lower()\n",
    "\n",
    "        if ext == '.pdf':\n",
    "            return self._process_pdf(file_path)\n",
    "        elif ext in ['.html', '.htm']:\n",
    "            return self._process_html(file_path)\n",
    "        elif ext in ['.txt', '.md']:\n",
    "            return self._process_text(file_path)\n",
    "        elif ext in ['.docx', '.doc']:\n",
    "            return self._process_word(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "    def _process_pdf(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process PDF documents.\"\"\"\n",
    "        document_structure = {\n",
    "            'metadata': {'source': file_path, 'type': 'pdf'},\n",
    "            'elements': []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Check if poppler is available\n",
    "            import shutil\n",
    "            if not shutil.which('pdftotext'):\n",
    "                print(\"Warning: poppler-utils not found in PATH. Installing...\")\n",
    "                import subprocess\n",
    "                subprocess.run([\"apt-get\", \"update\"], check=True)\n",
    "                subprocess.run([\"apt-get\", \"install\", \"-y\", \"poppler-utils\"], check=True)\n",
    "                print(\"poppler-utils installed successfully\")\n",
    "\n",
    "            # Read PDF and extract text\n",
    "            elements = partition_pdf(\n",
    "                filename=file_path,\n",
    "                extract_images=True,\n",
    "                infer_table_structure=True\n",
    "            )\n",
    "\n",
    "            # Extract tables separately using tabula\n",
    "            tables = tabula.read_pdf(file_path, pages='all')\n",
    "\n",
    "            # Process elements and categorize them\n",
    "            for element in elements:\n",
    "                elem_type = str(type(element)).lower()\n",
    "                element_data = {\n",
    "                    'content': str(element),\n",
    "                    'type': None\n",
    "                }\n",
    "\n",
    "                if 'title' in elem_type or 'heading' in elem_type:\n",
    "                    element_data['type'] = 'heading'\n",
    "                elif 'table' in elem_type:\n",
    "                    element_data['type'] = 'table'\n",
    "                elif 'image' in elem_type:\n",
    "                    element_data['type'] = 'image' \n",
    "                elif 'text' in elem_type:\n",
    "                    # Check if it's strikeout or highlighted (would need more PDF-specific analysis)\n",
    "                    if '~~' in str(element) or '--' in str(element):\n",
    "                        element_data['type'] = 'strikeout'\n",
    "                    elif any(marker in str(element) for marker in ['**', '__', '>>']):\n",
    "                        element_data['type'] = 'highlight'\n",
    "                    else:\n",
    "                        element_data['type'] = 'paragraph'\n",
    "\n",
    "                document_structure['elements'].append(element_data)\n",
    "\n",
    "            # Add tables from tabula to our elements list\n",
    "            for i, table in enumerate(tables):\n",
    "                document_structure['elements'].append({\n",
    "                    'content': table,\n",
    "                    'type': 'table',\n",
    "                    'pandas_table': True,\n",
    "                    'table_id': i\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF: {e}\")\n",
    "            document_structure['elements'].append({\n",
    "                'content': f\"Error processing PDF file: {str(e)}\",\n",
    "                'type': 'paragraph',\n",
    "        })\n",
    "\n",
    "        return document_structure\n",
    "\n",
    "    def _process_html(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process HTML documents.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        document_structure = {\n",
    "            'metadata': {'source': file_path, 'type': 'html'},\n",
    "            'elements': []\n",
    "        }\n",
    "\n",
    "        # Extract headings\n",
    "        for heading_level in range(1, 7):\n",
    "            for heading in soup.find_all(f'h{heading_level}'):\n",
    "                document_structure['elements'].append({\n",
    "                    'content': heading.get_text(),\n",
    "                    'type': 'heading',\n",
    "                    'level': heading_level\n",
    "                })\n",
    "\n",
    "        # Extract paragraphs\n",
    "        for para in soup.find_all('p'):\n",
    "            # Check for highlighted text\n",
    "            highlighted = para.find_all(['strong', 'b', 'mark', 'em'])\n",
    "            strikeout = para.find_all('s')\n",
    "\n",
    "            if highlighted:\n",
    "                for h in highlighted:\n",
    "                    document_structure['elements'].append({\n",
    "                        'content': h.get_text(),\n",
    "                        'type': 'highlight',\n",
    "                    })\n",
    "\n",
    "            if strikeout:\n",
    "                for s in strikeout:\n",
    "                    document_structure['elements'].append({\n",
    "                        'content': s.get_text(),\n",
    "                        'type': 'strikeout',\n",
    "                    })\n",
    "\n",
    "            # Add the full paragraph too\n",
    "            document_structure['elements'].append({\n",
    "                'content': para.get_text(),\n",
    "                'type': 'paragraph',\n",
    "            })\n",
    "\n",
    "        # Extract tables\n",
    "        for table in soup.find_all('table'):\n",
    "            # Convert HTML table to pandas DataFrame\n",
    "            table_data = []\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cols = row.find_all(['td', 'th'])\n",
    "                cols = [ele.get_text().strip() for ele in cols]\n",
    "                table_data.append(cols)\n",
    "\n",
    "            if table_data:\n",
    "                # Try to create a pandas DataFrame\n",
    "                try:\n",
    "                    df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "                    document_structure['elements'].append({\n",
    "                        'content': df,\n",
    "                        'type': 'table',\n",
    "                        'pandas_table': True\n",
    "                    })\n",
    "                except:\n",
    "                    # Fallback to string representation\n",
    "                    document_structure['elements'].append({\n",
    "                        'content': str(table_data),\n",
    "                        'type': 'table',\n",
    "                        'pandas_table': False\n",
    "                    })\n",
    "\n",
    "        # Extract images\n",
    "        for img in soup.find_all('img'):\n",
    "            document_structure['elements'].append({\n",
    "                'content': img.get('alt', 'Image') + f\" (src: {img.get('src', '')})\",\n",
    "                'type': 'image',\n",
    "            })\n",
    "\n",
    "        return document_structure\n",
    "\n",
    "    def _process_text(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process plain text or markdown documents.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        document_structure = {\n",
    "            'metadata': {'source': file_path, 'type': 'text'},\n",
    "            'elements': []\n",
    "        }\n",
    "\n",
    "        # Split by double newlines to separate paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "\n",
    "            # Check if it's a heading (starts with # in markdown)\n",
    "            if para.startswith('#'):\n",
    "                level = len(re.match(r'^#+', para).group())\n",
    "                heading_text = para.lstrip('#').strip()\n",
    "                document_structure['elements'].append({\n",
    "                    'content': heading_text,\n",
    "                    'type': 'heading',\n",
    "                    'level': level\n",
    "                })\n",
    "            # Check if it's a table (simple detection for markdown tables)\n",
    "            elif '|' in para and '-+-' in para.replace('|', '+'):\n",
    "                document_structure['elements'].append({\n",
    "                    'content': para,\n",
    "                    'type': 'table',\n",
    "                    'pandas_table': False\n",
    "                })\n",
    "            # Check for strikeout text (~~text~~ in markdown)\n",
    "            elif '~~' in para:\n",
    "                document_structure['elements'].append({\n",
    "                    'content': para,\n",
    "                    'type': 'strikeout',\n",
    "                })\n",
    "            # Check for highlighted text (** or __ in markdown)\n",
    "            elif '**' in para or '__' in para:\n",
    "                document_structure['elements'].append({\n",
    "                    'content': para,\n",
    "                    'type': 'highlight',\n",
    "                })\n",
    "            # Regular paragraph\n",
    "            else:\n",
    "                document_structure['elements'].append({\n",
    "                    'content': para,\n",
    "                    'type': 'paragraph',\n",
    "                })\n",
    "\n",
    "        return document_structure\n",
    "\n",
    "    def _process_word(self, file_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process Word documents using python-docx library.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the Word document\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with structured document content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import docx\n",
    "            from docx.oxml.table import CT_Tbl\n",
    "            from docx.oxml.text.paragraph import CT_P\n",
    "            \n",
    "            document = docx.Document(file_path)\n",
    "            document_structure = {\n",
    "                'metadata': {'source': file_path, 'type': 'docx'},\n",
    "                'elements': []\n",
    "            }\n",
    "            \n",
    "            # Process document body\n",
    "            for element in document.element.body:\n",
    "                # Process paragraphs\n",
    "                if isinstance(element, CT_P):\n",
    "                    paragraph = docx.Document().add_paragraph()\n",
    "                    paragraph._p = element\n",
    "                    text = paragraph.text.strip()\n",
    "                    \n",
    "                    if not text:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if it's likely a heading (based on style)\n",
    "                    p = document.add_paragraph()\n",
    "                    p._p = element\n",
    "                    if hasattr(p, 'style') and p.style and 'heading' in p.style.name.lower():\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': text,\n",
    "                            'type': 'heading',\n",
    "                            'level': int(p.style.name[-1]) if p.style.name[-1].isdigit() else 1\n",
    "                        })\n",
    "                    # Check for highlighted or strikeout text\n",
    "                    elif '**' in text or '__' in text:\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': text,\n",
    "                            'type': 'highlight'\n",
    "                        })\n",
    "                    elif '~~' in text:\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': text,\n",
    "                            'type': 'strikeout'\n",
    "                        })\n",
    "                    else:\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': text,\n",
    "                            'type': 'paragraph'\n",
    "                        })\n",
    "                \n",
    "                # Process tables\n",
    "                elif isinstance(element, CT_Tbl):\n",
    "                    table = docx.Document().add_table(rows=1, cols=1)\n",
    "                    table._tbl = element\n",
    "                    \n",
    "                    # Convert table to pandas DataFrame\n",
    "                    data = []\n",
    "                    headers = []\n",
    "                    \n",
    "                    # Get headers from first row\n",
    "                    if table.rows:\n",
    "                        for cell in table.rows[0].cells:\n",
    "                            headers.append(cell.text.strip())\n",
    "                    \n",
    "                    # Get data from remaining rows\n",
    "                    for row in table.rows[1:]:\n",
    "                        row_data = []\n",
    "                        for cell in row.cells:\n",
    "                            row_data.append(cell.text.strip())\n",
    "                        data.append(row_data)\n",
    "                    \n",
    "                    # Create pandas DataFrame if possible\n",
    "                    try:\n",
    "                        if headers and data:\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                            document_structure['elements'].append({\n",
    "                                'content': df,\n",
    "                                'type': 'table',\n",
    "                                'pandas_table': True\n",
    "                            })\n",
    "                        else:\n",
    "                            # Create simple text representation for table\n",
    "                            table_text = \"Table content:\\n\"\n",
    "                            for row in table.rows:\n",
    "                                row_text = [cell.text.strip() for cell in row.cells]\n",
    "                                table_text += \" | \".join(row_text) + \"\\n\"\n",
    "                            document_structure['elements'].append({\n",
    "                                'content': table_text,\n",
    "                                'type': 'table',\n",
    "                                'pandas_table': False\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting table to DataFrame: {e}\")\n",
    "                        table_text = \"Table content (error converting):\\n\"\n",
    "                        for row in table.rows:\n",
    "                            row_text = [cell.text.strip() for cell in row.cells]\n",
    "                            table_text += \" | \".join(row_text) + \"\\n\"\n",
    "                        document_structure['elements'].append({\n",
    "                            'content': table_text,\n",
    "                            'type': 'table',\n",
    "                            'pandas_table': False\n",
    "                        })\n",
    "            \n",
    "            # Process document properties (metadata)\n",
    "            try:\n",
    "                core_properties = document.core_properties\n",
    "                document_structure['metadata']['title'] = core_properties.title\n",
    "                document_structure['metadata']['author'] = core_properties.author\n",
    "                document_structure['metadata']['created'] = str(core_properties.created)\n",
    "                document_structure['metadata']['modified'] = str(core_properties.modified)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting document properties: {e}\")\n",
    "            \n",
    "            return document_structure\n",
    "        \n",
    "        except ImportError:\n",
    "            print(\"python-docx package not found. Please install it with 'pip install python-docx'\")\n",
    "            return {\n",
    "                'metadata': {'source': file_path, 'type': 'docx'},\n",
    "                'elements': [{\n",
    "                    'content': f\"python-docx package required for Word processing: {file_path}\",\n",
    "                    'type': 'paragraph',\n",
    "                }]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Word document: {e}\")\n",
    "            return {\n",
    "                'metadata': {'source': file_path, 'type': 'docx'},\n",
    "                'elements': [{\n",
    "                    'content': f\"Error processing Word document: {file_path}. Error: {str(e)}\",\n",
    "                    'type': 'paragraph',\n",
    "                }]\n",
    "            }\n",
    "\n",
    "    def extract_tables_as_triplets(self, document_structure: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract tables from document and convert to triplets for Neo4j.\n",
    "        \"\"\"\n",
    "        triplets = []\n",
    "        table_count = 0\n",
    "        \n",
    "        try:\n",
    "            for idx, element in enumerate(document_structure['elements']):\n",
    "                if element['type'] == 'table':\n",
    "                    if element.get('pandas_table', False):\n",
    "                        try:\n",
    "                            df = element['content']\n",
    "                            if isinstance(df, pd.DataFrame):\n",
    "                                # Get table name\n",
    "                                table_name = f\"Table_{table_count}\"\n",
    "                                table_count += 1\n",
    "                                \n",
    "                                # Look for the closest heading before this table\n",
    "                                for j in range(idx-1, -1, -1):\n",
    "                                    if j < len(document_structure['elements']) and document_structure['elements'][j]['type'] == 'heading':\n",
    "                                        table_name = document_structure['elements'][j]['content']\n",
    "                                        break\n",
    "                                        \n",
    "                                # Create triplets from table\n",
    "                                row_count = 0\n",
    "                                for _, row in df.iterrows():\n",
    "                                    for col in df.columns:\n",
    "                                        try:\n",
    "                                            triplets.append({\n",
    "                                                'subject': f\"{table_name}\",\n",
    "                                                'predicate': str(col),\n",
    "                                                'object': str(row[col]),\n",
    "                                                'row_id': row_count\n",
    "                                            })\n",
    "                                        except Exception as e:\n",
    "                                            print(f\"Error creating triplet for cell: {e}\")\n",
    "                                    row_count += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing table at index {idx}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_tables_as_triplets: {e}\")\n",
    "            \n",
    "        return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6390dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 3. Implementing Chunking Strategies\n",
    "# Now we'll develop functions for various chunking strategies:\n",
    "# 1. Title + Content (Hierarchical) chunking\n",
    "# 2. Paragraph-Based chunking\n",
    "# 3. Table-Aware chunking for Neo4j Graph representation\n",
    "# 4. Metadata-Aware chunking\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"Class implementing various chunking strategies for document processing.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize text splitters for different strategies\n",
    "        self.recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400,\n",
    "            chunk_overlap=100,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        self.md_header_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=[\n",
    "                (\"#\", \"header1\"),\n",
    "                (\"##\", \"header2\"),\n",
    "                (\"###\", \"header3\"),\n",
    "                (\"####\", \"header4\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.html_header_splitter = HTMLHeaderTextSplitter(\n",
    "            headers_to_split_on=[\n",
    "                (\"h1\", \"header1\"),\n",
    "                (\"h2\", \"header2\"),\n",
    "                (\"h3\", \"header3\"),\n",
    "                (\"h4\", \"header4\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def hierarchical_chunking(self, document_structure: Dict) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform Title + Content (Hierarchical) chunking.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        # First convert the document structure to a format suitable for hierarchical processing\n",
    "        markdown_content = self._convert_to_markdown_with_headers(document_structure)\n",
    "\n",
    "        # Split using the markdown header splitter\n",
    "        md_header_splits = self.md_header_splitter.split_text(markdown_content)\n",
    "\n",
    "        # Further split long content chunks if necessary\n",
    "        final_chunks = []\n",
    "        for doc in md_header_splits:\n",
    "            # If content is too long, split further\n",
    "            content = doc.page_content\n",
    "            if len(content) > 1000:  # adjust threshold as needed\n",
    "                sub_chunks = self.recursive_splitter.split_text(content)\n",
    "                # Copy metadata to each sub-chunk\n",
    "                for chunk in sub_chunks:\n",
    "                    final_chunks.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={**doc.metadata, 'chunk_type': 'hierarchical'}\n",
    "                    ))\n",
    "            else:\n",
    "                doc.metadata['chunk_type'] = 'hierarchical'\n",
    "                final_chunks.append(doc)\n",
    "\n",
    "        return final_chunks\n",
    "\n",
    "    def paragraph_based_chunking(self, document_structure: Dict) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform Paragraph-Based chunking with line breaks preserved.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "\n",
    "        # Extract paragraphs from the document\n",
    "        paragraphs = []\n",
    "        current_section = {'title': 'Document', 'content': ''}\n",
    "\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'heading':\n",
    "                # If we have content in the current section, save it\n",
    "                if current_section['content'].strip():\n",
    "                    paragraphs.append(current_section)\n",
    "\n",
    "                # Start a new section\n",
    "                current_section = {\n",
    "                    'title': element['content'],\n",
    "                    'content': ''\n",
    "                }\n",
    "            elif element['type'] == 'paragraph':\n",
    "                current_section['content'] += element['content'] + \"\\n\\n\"\n",
    "\n",
    "        # Add the last section if it has content\n",
    "        if current_section['content'].strip():\n",
    "            paragraphs.append(current_section)\n",
    "\n",
    "        # Create Document objects for each paragraph\n",
    "        for para in paragraphs:\n",
    "            # Split content if it's too long\n",
    "            if len(para['content']) > 1000:\n",
    "                sub_chunks = self.recursive_splitter.split_text(para['content'])\n",
    "                for i, chunk in enumerate(sub_chunks):\n",
    "                    chunks.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            'title': para['title'],\n",
    "                            'chunk_type': 'paragraph',\n",
    "                            'chunk_index': i,\n",
    "                            'source': document_structure['metadata']['source']\n",
    "                        }\n",
    "                    ))\n",
    "            else:\n",
    "                chunks.append(Document(\n",
    "                    page_content=para['content'],\n",
    "                    metadata={\n",
    "                        'title': para['title'],\n",
    "                        'chunk_type': 'paragraph',\n",
    "                        'source': document_structure['metadata']['source']\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def table_aware_chunking(self, document_structure: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform Table-Aware chunking for Neo4j graph.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries representing table triplets for Neo4j\n",
    "        \"\"\"\n",
    "        # Extract tables and convert to triplets\n",
    "        analyzer = DocumentAnalyzer()\n",
    "        return analyzer.extract_tables_as_triplets(document_structure)\n",
    "\n",
    "    def metadata_aware_chunking(self, document_structure: Dict) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform Metadata-Aware chunking with Milvus compatibility.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Identify semantic sections\n",
    "        semantic_sections = self._identify_semantic_sections(document_structure)\n",
    "        \n",
    "        for section in semantic_sections:\n",
    "            # Calculate chunk size based on content type\n",
    "            if section['type'] == 'rule_explanation':\n",
    "                chunk_size = 1500\n",
    "            elif section['type'] == 'user_scenario':\n",
    "                chunk_size = 1000\n",
    "            elif section['type'] == 'business_logic':\n",
    "                chunk_size = 800\n",
    "            else:\n",
    "                chunk_size = 1000\n",
    "                \n",
    "            custom_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=min(100, chunk_size // 10),\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            # Split content\n",
    "            text_chunks = custom_splitter.split_text(section['content'])\n",
    "            \n",
    "            # Convert keywords and categories to strings for Milvus compatibility\n",
    "            keywords_str = \",\".join(section.get('keywords', []))\n",
    "            categories_str = \",\".join(section.get('categories', []))\n",
    "            \n",
    "            # Create Document objects with compatible metadata\n",
    "            for i, chunk in enumerate(text_chunks):\n",
    "                chunks.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        'title': section.get('title', 'Untitled Section'),\n",
    "                        'content_type': section['type'],\n",
    "                        'source': document_structure['metadata']['source'],\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(text_chunks),\n",
    "                        'chunk_type': 'metadata_aware',\n",
    "                        # Store keywords and categories as strings\n",
    "                        'keywords_str': keywords_str,\n",
    "                        'categories_str': categories_str\n",
    "                    }\n",
    "                ))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _convert_to_markdown_with_headers(self, document_structure: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Convert document structure to markdown format with headers.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            Markdown string representation of the document\n",
    "        \"\"\"\n",
    "        markdown = []\n",
    "        current_heading = \"\"\n",
    "\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'heading':\n",
    "                level = element.get('level', 1)\n",
    "                heading = '#' * level + ' ' + element['content']\n",
    "                markdown.append(heading)\n",
    "                current_heading = element['content']\n",
    "            elif element['type'] == 'paragraph':\n",
    "                markdown.append(element['content'])\n",
    "            elif element['type'] == 'table':\n",
    "                if isinstance(element['content'], pd.DataFrame):\n",
    "                    table_str = element['content'].to_markdown()\n",
    "                    markdown.append(table_str)\n",
    "                else:\n",
    "                    markdown.append(str(element['content']))\n",
    "            elif element['type'] in ['strikeout', 'highlight', 'image']:\n",
    "                markdown.append(element['content'])\n",
    "\n",
    "        return '\\n\\n'.join(markdown)\n",
    "\n",
    "    def _identify_semantic_sections(self, document_structure: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Identify semantic sections in the document.\n",
    "\n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "\n",
    "        Returns:\n",
    "            List of semantic section dictionaries\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        current_section = None\n",
    "\n",
    "        # Patterns to identify content types\n",
    "        rule_patterns = ['rule', 'regulation', 'policy', 'guidelines']\n",
    "        logic_patterns = ['workflow', 'process', 'logic', 'procedure', 'business logic']\n",
    "        scenario_patterns = ['scenario', 'example', 'use case', 'user story']\n",
    "        interaction_patterns = ['interaction', 'interface', 'system', 'integration', 'api']\n",
    "\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'heading':\n",
    "                # If there's a current section with content, save it\n",
    "                if current_section and current_section.get('content'):\n",
    "                    sections.append(current_section)\n",
    "\n",
    "                # Determine the section type based on heading content\n",
    "                heading_lower = element['content'].lower()\n",
    "\n",
    "                if any(pattern in heading_lower for pattern in rule_patterns):\n",
    "                    section_type = 'rule_explanation'\n",
    "                elif any(pattern in heading_lower for pattern in logic_patterns):\n",
    "                    section_type = 'business_logic'\n",
    "                elif any(pattern in heading_lower for pattern in scenario_patterns):\n",
    "                    section_type = 'user_scenario'\n",
    "                elif any(pattern in heading_lower for pattern in interaction_patterns):\n",
    "                    section_type = 'system_interaction'\n",
    "                else:\n",
    "                    section_type = 'general'\n",
    "\n",
    "                # Create a new section\n",
    "                current_section = {\n",
    "                    'title': element['content'],\n",
    "                    'type': section_type,\n",
    "                    'content': '',\n",
    "                    'keywords': self._extract_keywords(element['content']),\n",
    "                    'categories': [section_type]\n",
    "                }\n",
    "            elif element['type'] in ['paragraph', 'strikeout', 'highlight']:\n",
    "                if current_section:\n",
    "                    current_section['content'] += element['content'] + \"\\n\\n\"\n",
    "                else:\n",
    "                    # Create a default section if none exists\n",
    "                    current_section = {\n",
    "                        'title': 'Introduction',\n",
    "                        'type': 'general',\n",
    "                        'content': element['content'] + \"\\n\\n\",\n",
    "                        'keywords': [],\n",
    "                        'categories': ['general']\n",
    "                    }\n",
    "            elif element['type'] == 'table':\n",
    "                # Tables are handled separately for Neo4j, but we still include their text representation\n",
    "                # in the content for completeness\n",
    "                if current_section:\n",
    "                    if isinstance(element['content'], pd.DataFrame):\n",
    "                        current_section['content'] += \"Table content:\\n\"\n",
    "                        current_section['content'] += str(element['content']) + \"\\n\\n\"\n",
    "                    else:\n",
    "                        current_section['content'] += \"Table content:\\n\" + str(element['content']) + \"\\n\\n\"\n",
    "\n",
    "        # Add the last section if it exists\n",
    "        if current_section and current_section.get('content'):\n",
    "            sections.append(current_section)\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def _extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract keywords from text.\n",
    "\n",
    "        Args:\n",
    "            text: Text to extract keywords from\n",
    "\n",
    "        Returns:\n",
    "            List of keywords\n",
    "        \"\"\"\n",
    "        # Simple implementation - extract meaningful words and filter stop words\n",
    "        stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'if', 'then', 'else', 'when', \n",
    "                     'on', 'in', 'at', 'to', 'for', 'with', 'by', 'about', 'as', 'of'}\n",
    "\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "        keywords = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Return unique keywords\n",
    "        return list(set(keywords))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43fae7a",
   "metadata": {},
   "source": [
    "### 4. Setting Up the Embedding Model\n",
    " We'll configure a local embedding model to generate vector representations of our chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b2b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Class to manage embeddings for document chunks.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the HuggingFace model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        print(f\"Initialized embedding model: {model_name}\")\n",
    "\n",
    "    def generate_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate an embedding for text.\n",
    "\n",
    "        Args:\n",
    "            text: Text to embed\n",
    "\n",
    "        Returns:\n",
    "            List of floats representing the embedding vector\n",
    "        \"\"\"\n",
    "        # The HuggingFaceEmbeddings class follows the LangChain embedding interface\n",
    "        return self.embeddings.embed_query(text)\n",
    "\n",
    "    def batch_embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Batch embed multiple texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of texts to embed\n",
    "\n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        return self.embeddings.embed_documents(texts)\n",
    "\n",
    "    def embed_documents(self, documents: List[Document]) -> Tuple[List[List[float]], List[Document]]:\n",
    "        \"\"\"\n",
    "        Embed LangChain Document objects.\n",
    "\n",
    "        Args:\n",
    "            documents: List of Document objects to embed\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (embeddings, documents)\n",
    "        \"\"\"\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.batch_embed(texts)\n",
    "        return embeddings, documents\n",
    "\n",
    "    def embed_triplets(self, triplets: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Embed triplets for Neo4j graph.\n",
    "\n",
    "        Args:\n",
    "            triplets: List of triplet dictionaries\n",
    "\n",
    "        Returns:\n",
    "            Triplets with embeddings added\n",
    "        \"\"\"\n",
    "        for triplet in triplets:\n",
    "            # Create a concatenated text representation of the triplet\n",
    "            triplet_text = f\"{triplet['subject']} {triplet['predicate']} {triplet['object']}\"\n",
    "            triplet['embedding'] = self.generate_embedding(triplet_text)\n",
    "\n",
    "        return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4f23a",
   "metadata": {},
   "source": [
    "### 5. Configuring Vector Databases (Milvus and Neo4j)\n",
    "Set up connections and schemas for Milvus and Neo4j databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0894ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDBManager:\n",
    "    \"\"\"Class to manage vector database connections and operations.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"Initialize with the embedding manager.\"\"\"\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.milvus_client = None\n",
    "        self.neo4j_client = None\n",
    "\n",
    "        # Collections in Milvus (document_id -> collection)\n",
    "        self.collections = {}\n",
    "        \n",
    "        # Document metadata store\n",
    "        self.doc_metadata = {}\n",
    "\n",
    "    def setup_milvus(self, host='localhost', port='2379'):\n",
    "        \"\"\"Set up connection to Milvus.\"\"\"\n",
    "        try:\n",
    "            # Define Milvus connection parameters\n",
    "            self.milvus_connection_params = {\n",
    "                \"host\": host,\n",
    "                \"port\": port\n",
    "            }\n",
    "            \n",
    "            print(\"Milvus connection parameters configured.\")\n",
    "            self.milvus_available = True\n",
    "            \n",
    "            # Check if connection works\n",
    "            from pymilvus import connections\n",
    "            connections.connect(host=host, port=port)\n",
    "            print(\"Milvus connection test successful\")\n",
    "            connections.disconnect(alias=\"default\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up Milvus: {e}\")\n",
    "            self.milvus_available = False\n",
    "\n",
    "    def setup_neo4j(self, uri='bolt://localhost:7687', username='neo4j', password='venev'):\n",
    "        \"\"\"Set up connection to Neo4j.\"\"\"\n",
    "        try:\n",
    "            # Store Neo4j connection parameters\n",
    "            self.neo4j_connection_params = {\n",
    "                \"url\": uri,\n",
    "                \"username\": username,\n",
    "                \"password\": password\n",
    "            }\n",
    "            \n",
    "            # Initialize Neo4j graph client\n",
    "            self.neo4j_client = Neo4jGraph(\n",
    "                url=uri,\n",
    "                username=username,\n",
    "                password=password\n",
    "            )\n",
    "            \n",
    "            # Initialize Neo4j vector client\n",
    "            self.neo4j_vector = Neo4jVector(\n",
    "                url=uri,\n",
    "                username=username,\n",
    "                password=password,\n",
    "                embedding=self.embedding_manager.embeddings,\n",
    "                index_name=\"documentVectors\",\n",
    "                node_label=\"DocumentChunk\"\n",
    "            )\n",
    "            \n",
    "            # Set up schema for our document graph (only once)\n",
    "            self._create_neo4j_schema()\n",
    "            print(\"Neo4j connection established and schema created.\")\n",
    "            self.neo4j_available = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up Neo4j: {e}\")\n",
    "            self.neo4j_client = None\n",
    "            self.neo4j_vector = None\n",
    "            self.neo4j_available = False\n",
    "\n",
    "    def _create_neo4j_schema(self):\n",
    "        \"\"\"Create Neo4j schema for document storage.\"\"\"\n",
    "        # Create constraints that work with Community Edition\n",
    "        self.neo4j_client.query(\"\"\"\n",
    "            CREATE CONSTRAINT document_id IF NOT EXISTS\n",
    "            FOR (d:Document) REQUIRE d.id IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        self.neo4j_client.query(\"\"\"\n",
    "            CREATE CONSTRAINT chunk_id IF NOT EXISTS\n",
    "            FOR (c:DocumentChunk) REQUIRE c.id IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create vector index if needed\n",
    "        self.neo4j_client.query(\"\"\"\n",
    "            CREATE VECTOR INDEX documentVectors IF NOT EXISTS\n",
    "            FOR (c:DocumentChunk) \n",
    "            ON c.embedding\n",
    "            OPTIONS {indexConfig: {\n",
    "              `vector.dimensions`: 384,\n",
    "              `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "        \"\"\")\n",
    "\n",
    "    def store_in_milvus(self, documents: List[Document], doc_id: str, content_type: str):\n",
    "        \"\"\"Store documents in Milvus using doc_id for collection naming.\"\"\"\n",
    "        if not hasattr(self, 'milvus_available') or not self.milvus_available:\n",
    "            print(\"Skipping Milvus storage - Milvus not available\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Sanitize the document ID and content type to create a valid collection name\n",
    "            # Replace spaces and special characters with underscores\n",
    "            sanitized_doc_id = re.sub(r'[^a-zA-Z0-9_]', '_', doc_id)\n",
    "            sanitized_content_type = re.sub(r'[^a-zA-Z0-9_]', '_', content_type)\n",
    "            \n",
    "            # Create a collection name that includes sanitized document ID and content type\n",
    "            collection_name = f\"doc_{sanitized_doc_id}_{sanitized_content_type}\"\n",
    "            \n",
    "            print(f\"Using sanitized collection name: {collection_name}\")\n",
    "            \n",
    "            # Generate embeddings for documents\n",
    "            embeddings = [self.embedding_manager.generate_embedding(doc.page_content) for doc in documents]\n",
    "\n",
    "            # Create Milvus collection\n",
    "            vector_store = Milvus.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=self.embedding_manager.embeddings,\n",
    "                collection_name=collection_name,\n",
    "                connection_args=self.milvus_connection_params\n",
    "            )\n",
    "\n",
    "            # Store the collection reference with original doc_id\n",
    "            if doc_id not in self.collections:\n",
    "                self.collections[doc_id] = {}\n",
    "            \n",
    "            self.collections[doc_id][content_type] = vector_store\n",
    "\n",
    "            print(f\"Stored {len(documents)} documents in Milvus collection: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing documents in Milvus: {e}\")\n",
    "\n",
    "    def store_in_neo4j(self, triplets: List[Dict], doc_id: str):\n",
    "        \"\"\"Store triplets in Neo4j with document ID.\"\"\"\n",
    "        if not hasattr(self, 'neo4j_available') or not self.neo4j_available:\n",
    "            print(\"Skipping Neo4j storage - Neo4j not available\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Get the Neo4j driver session properly\n",
    "            session = self.neo4j_client._driver.session()\n",
    "            \n",
    "            # Process in smaller batches for better error handling\n",
    "            batch_size = 50\n",
    "            for i in range(0, len(triplets), batch_size):\n",
    "                batch = triplets[i:i+batch_size]\n",
    "                try:\n",
    "                    # Begin a transaction for this batch\n",
    "                    with session.begin_transaction() as tx:\n",
    "                        # Convert parameters for Neo4j\n",
    "                        params = {\n",
    "                            \"batch_size\": len(batch),\n",
    "                            \"subjects\": [t[\"subject\"] for t in batch],\n",
    "                            \"predicates\": [t[\"predicate\"] for t in batch],\n",
    "                            \"objects\": [t[\"object\"] for t in batch],\n",
    "                            \"embeddings\": [t[\"embedding\"] for t in batch],\n",
    "                            \"doc_id\": doc_id\n",
    "                        }\n",
    "\n",
    "                        # Enhanced Cypher query with document ID\n",
    "                        query = \"\"\"\n",
    "                        UNWIND range(0, $batch_size - 1) as i\n",
    "                        MERGE (d:Document {id: $doc_id})\n",
    "                        MERGE (s:Subject {name: $subjects[i], doc_id: $doc_id})\n",
    "                        MERGE (o:Object {name: $objects[i], doc_id: $doc_id})\n",
    "                        WITH d, s, o, i\n",
    "                        CREATE (s)-[r:HAS_PROPERTY {name: $predicates[i]}]->(o)\n",
    "                        CREATE (s)-[:BELONGS_TO]->(d)\n",
    "                        CREATE (o)-[:BELONGS_TO]->(d)\n",
    "                        WITH d, s, o, i\n",
    "                        CREATE (c:DocumentChunk {\n",
    "                            id: randomUUID(),\n",
    "                            text: $subjects[i] + ' ' + $predicates[i] + ' ' + $objects[i],\n",
    "                            subject: $subjects[i],\n",
    "                            predicate: $predicates[i],\n",
    "                            object: $objects[i],\n",
    "                            doc_id: $doc_id\n",
    "                        })\n",
    "                        SET c.embedding = $embeddings[i]\n",
    "                        CREATE (c)-[:SUBJECT_OF]->(s)\n",
    "                        CREATE (c)-[:OBJECT_OF]->(o)\n",
    "                        CREATE (c)-[:PART_OF]->(d)\n",
    "                        \"\"\"\n",
    "\n",
    "                        # Execute query within transaction\n",
    "                        result = tx.run(query, params)\n",
    "                        result.consume()  # Ensure execution completes\n",
    "                    \n",
    "                    print(f\"Processed batch {i//batch_size + 1}/{(len(triplets) + batch_size - 1)//batch_size}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "                    # Continue to the next batch even if this one fails\n",
    "            \n",
    "            # Close the session when done\n",
    "            session.close()\n",
    "            print(\"Stored triplets in Neo4j (completed batches)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Neo4j transaction: {e}\")\n",
    "            # Make sure session is closed if an error occurs\n",
    "            if 'session' in locals():\n",
    "                session.close()\n",
    "\n",
    "    def register_document_metadata(self, doc_id: str, metadata: Dict):\n",
    "        \"\"\"Register document metadata for later reference.\"\"\"\n",
    "        self.doc_metadata[doc_id] = metadata\n",
    "        \n",
    "    def get_document_metadata(self, doc_id: str) -> Dict:\n",
    "        \"\"\"Get metadata for a document.\"\"\"\n",
    "        return self.doc_metadata.get(doc_id, {})\n",
    "        \n",
    "    def list_documents(self) -> List[str]:\n",
    "        \"\"\"List all documents in the system.\"\"\"\n",
    "        return list(self.collections.keys())\n",
    "\n",
    "    def document_exists(self, doc_id: str) -> bool:\n",
    "        \"\"\"Check if document already exists in the system.\"\"\"\n",
    "        return doc_id in self.collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b92d83",
   "metadata": {},
   "source": [
    "### 6. Document Processing Pipeline\n",
    "Now let's develop a pipeline that processes documents by content type, applies appropriate chunking strategies, and stores data in the correct database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef1cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessingPipeline:\n",
    "    \"\"\"Pipeline for document processing, chunking, and storage.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        analyzer: DocumentAnalyzer, \n",
    "        chunker: DocumentChunker, \n",
    "        embedding_manager: EmbeddingManager,\n",
    "        vector_db_manager: VectorDBManager\n",
    "    ):\n",
    "        self.analyzer = analyzer\n",
    "        self.chunker = chunker\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.vector_db_manager = vector_db_manager\n",
    "\n",
    "        # Content type mapping\n",
    "        self.content_mapping = {\n",
    "            'rule_explanation': {'chunking': 'hierarchical', 'storage': 'milvus'},\n",
    "            'business_logic': {'chunking': 'paragraph', 'storage': 'milvus'},\n",
    "            'table': {'chunking': 'table', 'storage': 'neo4j'},\n",
    "            'user_scenario': {'chunking': 'paragraph', 'storage': 'milvus'},\n",
    "            'system_interaction': {'chunking': 'metadata', 'storage': 'both'}\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str, doc_id: str = None):\n",
    "        \"\"\"\n",
    "        Process a document file through the entire pipeline.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "            doc_id: Optional document ID, defaults to file basename\n",
    "        \"\"\"\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path).split('.')[0]\n",
    "            \n",
    "        print(f\"Processing document: {file_path} (ID: {doc_id})\")\n",
    "        \n",
    "        # Check if document has already been processed\n",
    "        if self.vector_db_manager.document_exists(doc_id):\n",
    "            print(f\"Document {doc_id} has already been processed. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # Step 1: Analyze document\n",
    "        document_structure = self.analyzer.load_document(file_path)\n",
    "        print(f\"Document analysis complete. Found {len(document_structure['elements'])} elements.\")\n",
    "\n",
    "        # Store document metadata\n",
    "        document_structure['metadata']['doc_id'] = doc_id\n",
    "        self.vector_db_manager.register_document_metadata(doc_id, document_structure['metadata'])\n",
    "\n",
    "        # Step 2: Process each content type with appropriate chunking\n",
    "        content_types = self._identify_content_types(document_structure)\n",
    "        print(f\"Identified content types: {', '.join(content_types)}\")\n",
    "\n",
    "        # Step 3: Apply chunking strategies and store in appropriate databases\n",
    "        self._process_content_types(document_structure, content_types, doc_id)\n",
    "\n",
    "        print(f\"Document {doc_id} processing complete.\")\n",
    "        \n",
    "    # Add doc_id parameter to relevant methods\n",
    "    def _process_content_types(self, document_structure: Dict, content_types: List[str], doc_id: str):\n",
    "        \"\"\"Process content types with doc_id.\"\"\"\n",
    "        # Create a copy of content_types to avoid modification during iteration\n",
    "        types_to_process = content_types.copy()\n",
    "        \n",
    "        # Process tables first if present\n",
    "        if 'table' in types_to_process:\n",
    "            print(\"Processing tables...\")\n",
    "            try:\n",
    "                triplets = self.chunker.table_aware_chunking(document_structure)\n",
    "                \n",
    "                if triplets:\n",
    "                    # Process triplets with doc_id\n",
    "                    if hasattr(self.vector_db_manager, 'neo4j_available') and self.vector_db_manager.neo4j_available:\n",
    "                        try:\n",
    "                            triplets_with_embeddings = self.embedding_manager.embed_triplets(triplets)\n",
    "                            self.vector_db_manager.store_in_neo4j(triplets_with_embeddings, doc_id)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error storing triplets in Neo4j: {e}\")\n",
    "                            \n",
    "                    # Fallback to Milvus for redundancy\n",
    "                    try:\n",
    "                        print(\"Storing table data in Milvus for backup\")\n",
    "                        table_chunks = []\n",
    "                        for triplet in triplets:\n",
    "                            table_chunks.append(Document(\n",
    "                                page_content=f\"{triplet['subject']} - {triplet['predicate']}: {triplet['object']}\",\n",
    "                                metadata={\n",
    "                                    'title': triplet['subject'],\n",
    "                                    'chunk_type': 'table_fallback',\n",
    "                                    'source': document_structure['metadata']['source'],\n",
    "                                    'doc_id': doc_id\n",
    "                                }\n",
    "                            ))\n",
    "                        if table_chunks:\n",
    "                            self.vector_db_manager.store_in_milvus(table_chunks, doc_id, 'business_logic')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error storing table fallback in Milvus: {e}\")\n",
    "                else:\n",
    "                    print(\"No valid tables found for extraction\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing tables: {e}\")\n",
    "                \n",
    "            types_to_process.remove('table')\n",
    "        \n",
    "        # Process other content types with better error handling\n",
    "        for content_type in types_to_process:\n",
    "            print(f\"Processing content type: {content_type}\")\n",
    "            \n",
    "            try:\n",
    "                chunking_strategy = self.content_mapping[content_type]['chunking']\n",
    "                storage = self.content_mapping[content_type]['storage']\n",
    "                \n",
    "                # Apply chunking strategy with error handling\n",
    "                chunks = []\n",
    "                try:\n",
    "                    if chunking_strategy == 'hierarchical':\n",
    "                        chunks = self.chunker.hierarchical_chunking(document_structure)\n",
    "                    elif chunking_strategy == 'paragraph':\n",
    "                        chunks = self.chunker.paragraph_based_chunking(document_structure)\n",
    "                    elif chunking_strategy == 'metadata':\n",
    "                        chunks = self.chunker.metadata_aware_chunking(document_structure)\n",
    "                    else:\n",
    "                        print(f\"Unknown chunking strategy for {content_type}: {chunking_strategy}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Add doc_id to all chunks\n",
    "                    for chunk in chunks:\n",
    "                        chunk.metadata['doc_id'] = doc_id\n",
    "                        \n",
    "                    print(f\"Created {len(chunks)} chunks using {chunking_strategy} strategy\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying chunking strategy '{chunking_strategy}': {e}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Store in appropriate database with fallbacks\n",
    "                if not chunks:\n",
    "                    print(f\"No chunks created for {content_type}. Skipping storage.\")\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # Always try Milvus first for reliability\n",
    "                    if hasattr(self.vector_db_manager, 'milvus_available') and self.vector_db_manager.milvus_available:\n",
    "                        print(f\"Storing {len(chunks)} chunks in Milvus\")\n",
    "                        self.vector_db_manager.store_in_milvus(chunks, doc_id, content_type)\n",
    "                        \n",
    "                    # If Neo4j storage is required and available\n",
    "                    if (storage == 'neo4j' or storage == 'both'):\n",
    "                        try:\n",
    "                            print(\"Converting chunks to triplets for Neo4j storage\")\n",
    "                            triplets = self._convert_chunks_to_triplets(chunks, content_type)\n",
    "                            triplets_with_embeddings = self.embedding_manager.embed_triplets(triplets)\n",
    "                            \n",
    "                            # Try Neo4j storage with doc_id\n",
    "                            if hasattr(self.vector_db_manager, 'neo4j_available') and self.vector_db_manager.neo4j_available:\n",
    "                                try:\n",
    "                                    self.vector_db_manager.store_in_neo4j(triplets_with_embeddings, doc_id)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error storing in Neo4j: {e}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error preparing triplets: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error storing chunks for {content_type}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing content type {content_type}: {e}\")\n",
    "                \n",
    "    def _identify_content_types(self, document_structure: Dict) -> List[str]:\n",
    "        \"\"\"\n",
    "        Identify content types in the document.\n",
    "        \n",
    "        Args:\n",
    "            document_structure: Document structure dictionary\n",
    "            \n",
    "        Returns:\n",
    "            List of content types found in the document\n",
    "        \"\"\"\n",
    "        content_types = set()\n",
    "        \n",
    "        # First, check if we have tables\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'table':\n",
    "                content_types.add('table')\n",
    "                break\n",
    "        \n",
    "        # Analyze headings to determine content types\n",
    "        heading_texts = []\n",
    "        for element in document_structure['elements']:\n",
    "            if element['type'] == 'heading':\n",
    "                heading_text = element['content'].lower()\n",
    "                heading_texts.append(heading_text)\n",
    "                \n",
    "                # Look for keywords indicating content types\n",
    "                if any(pattern in heading_text for pattern in ['rule', 'regulation', 'policy']):\n",
    "                    content_types.add('rule_explanation')\n",
    "                \n",
    "                if any(pattern in heading_text for pattern in ['workflow', 'process', 'procedure']):\n",
    "                    content_types.add('business_logic')\n",
    "                    \n",
    "                if any(pattern in heading_text for pattern in ['scenario', 'example', 'use case']):\n",
    "                    content_types.add('user_scenario')\n",
    "                    \n",
    "                if any(pattern in heading_text for pattern in ['system', 'interaction', 'interface']):\n",
    "                    content_types.add('system_interaction')\n",
    "        \n",
    "        # If no specific content types are identified, add default ones\n",
    "        if not content_types:\n",
    "            # Default to treating it as business logic and rule explanation\n",
    "            content_types.add('business_logic')\n",
    "            \n",
    "        # Always add the table type if tables are detected\n",
    "        has_tables = any(element['type'] == 'table' for element in document_structure['elements'])\n",
    "        if has_tables:\n",
    "            content_types.add('table')\n",
    "            \n",
    "        return list(content_types)\n",
    "        \n",
    "    def _convert_chunks_to_triplets(self, chunks: List[Document], content_type: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Convert document chunks to triplets for Neo4j.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of document chunks\n",
    "            content_type: Type of content\n",
    "            \n",
    "        Returns:\n",
    "            List of triplets\n",
    "        \"\"\"\n",
    "        triplets = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            title = chunk.metadata.get('title', 'Untitled')\n",
    "            content = chunk.page_content\n",
    "            \n",
    "            # Create triplet - simple subject-predicate-object format\n",
    "            triplet = {\n",
    "                'subject': title,\n",
    "                'predicate': f\"has_{content_type}\",\n",
    "                'object': content[:200] if len(content) > 200 else content,  # Truncate long content\n",
    "                'chunk_id': i\n",
    "            }\n",
    "            \n",
    "            triplets.append(triplet)\n",
    "            \n",
    "        return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78acabbe",
   "metadata": {},
   "source": [
    "### 8. Query Processing with Gemini LLM\n",
    "Let's integrate the Gemini-1.5-flash-8b model via API to process queries against the retrieved document chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ed752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiProcessor:\n",
    "    \"\"\"Enhanced class for processing queries with Gemini LLM.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key=None):\n",
    "        \"\"\"Initialize the Gemini processor.\"\"\"\n",
    "        # Try to get API key from parameter, then environment variable\n",
    "        self.api_key = api_key or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            print(\"Warning: No API key provided for Gemini. Set GOOGLE_API_KEY environment variable.\")\n",
    "        else:\n",
    "            try:\n",
    "                genai.configure(api_key=self.api_key)\n",
    "                self.model = genai.GenerativeModel('gemini-1.5-flash-8b')\n",
    "                print(\"Gemini model initialized successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing Gemini: {e}\")\n",
    "                self.model = None\n",
    "    \n",
    "    def analyze_query_topics(self, query: str, num_topics: int = 3) -> List[str]:\n",
    "        \"\"\"Extract main topics from query to aid in document relevance.\"\"\"\n",
    "        if not self.model:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Identify the {num_topics} main topics or subjects in this query. \n",
    "            Return only a comma-separated list of topics without explanation.\n",
    "            \n",
    "            Query: {query}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.model.generate_content(prompt)\n",
    "            topics = [topic.strip() for topic in response.text.split(',')]\n",
    "            return topics[:num_topics]  # Ensure we don't exceed requested number\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing query topics: {e}\")\n",
    "            return []\n",
    "\n",
    "    def process_query(self, query: str, retrieved_docs: List[Document]) -> str:\n",
    "        \"\"\"Process a query using the Gemini LLM and retrieved documents.\"\"\"\n",
    "        if not self.model:\n",
    "            return \"Gemini model not initialized. Please provide a valid API key.\"\n",
    "\n",
    "        try:\n",
    "            # Prepare context with document metadata\n",
    "            context_elements = []\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                # Extract document ID and other useful metadata\n",
    "                doc_id = doc.metadata.get('doc_id', 'unknown')\n",
    "                source = doc.metadata.get('source', 'unknown')\n",
    "                content_type = doc.metadata.get('content_type', '')\n",
    "                title = doc.metadata.get('title', '')\n",
    "                \n",
    "                # Format the context entry\n",
    "                context_entry = f\"Document {i+1} [{doc_id}]:\\n\"\n",
    "                if title:\n",
    "                    context_entry += f\"Title: {title}\\n\"\n",
    "                if content_type:\n",
    "                    context_entry += f\"Content type: {content_type}\\n\"\n",
    "                context_entry += f\"Content: {doc.page_content}\\n\"\n",
    "                \n",
    "                context_elements.append(context_entry)\n",
    "                \n",
    "            context = \"\\n\\n\".join(context_elements)\n",
    "\n",
    "            # Enhanced prompt with metadata awareness\n",
    "            prompt = f\"\"\"\n",
    "            You are an intelligent assistant that answers questions based on provided context.\n",
    "            The context consists of information from various documents, each with its own ID and metadata.\n",
    "\n",
    "            Context information:\n",
    "            {context}\n",
    "            \n",
    "            User question: {query}\n",
    "            \n",
    "            Please answer the question based only on the provided context. If the context doesn't contain enough information\n",
    "            to fully answer the question, acknowledge what you can answer based on the context and what information is missing.\n",
    "            When referring to specific information, mention which document it comes from.\n",
    "            \"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            response = self.model.generate_content(prompt)\n",
    "            return response.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query with Gemini: {e}\")\n",
    "            return f\"Error processing query: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc0694",
   "metadata": {},
   "source": [
    "### 7. Hybrid Retrieval System with LangChain\n",
    "Hybrid retrieval system using LangChain v0.3 to combine search results from both Milvus and Neo4j.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "436aa28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"Class for hybrid retrieval from multiple vector databases.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_db_manager: VectorDBManager):\n",
    "        \"\"\"Initialize with the vector database manager.\"\"\"\n",
    "        self.vector_db_manager = vector_db_manager\n",
    "        self.retrievers = {}\n",
    "        self.doc_weights = {}  # For document-specific weighting\n",
    "        \n",
    "    def setup_langchain_retrieval(self, use_gemini_for_query_expansion=True):\n",
    "        \"\"\"Set up LangChain retrieval with smarter strategies.\"\"\"\n",
    "        try:\n",
    "            # Create retriever objects for different documents and collections\n",
    "            self.retrievers = {}\n",
    "            \n",
    "            # Add Milvus retrievers for each document\n",
    "            for doc_id, content_types in self.vector_db_manager.collections.items():\n",
    "                for content_type, collection in content_types.items():\n",
    "                    if collection is not None:\n",
    "                        try:\n",
    "                            retriever_name = f\"milvus_{doc_id}_{content_type}\"\n",
    "                            self.retrievers[retriever_name] = collection.as_retriever(\n",
    "                                search_type=\"similarity\",\n",
    "                                search_kwargs={\"k\": 3}\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error setting up retriever for {doc_id}/{content_type}: {e}\")\n",
    "\n",
    "            # Add Neo4j retriever if available\n",
    "            if hasattr(self.vector_db_manager, 'neo4j_vector') and self.vector_db_manager.neo4j_vector:\n",
    "                try:\n",
    "                    self.retrievers[\"neo4j\"] = self.vector_db_manager.neo4j_vector.as_retriever(\n",
    "                        search_kwargs={\"k\": 3}\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error setting up Neo4j retriever: {e}\")\n",
    "            \n",
    "            # Set up query expansion if requested\n",
    "            if use_gemini_for_query_expansion and self.retrievers:\n",
    "                try:\n",
    "                    if \"GOOGLE_API_KEY\" in os.environ and os.environ[\"GOOGLE_API_KEY\"]:\n",
    "                        # Set up Gemini for multi-query generation\n",
    "                        self.gemini_llm = ChatGoogleGenerativeAI(\n",
    "                            model=\"gemini-1.5-flash-8b\",\n",
    "                            temperature=0.2\n",
    "                        )\n",
    "\n",
    "                        # Use first available retriever for multi-query\n",
    "                        first_retriever_name = list(self.retrievers.keys())[0]\n",
    "                        try:\n",
    "                            self.multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "                                retriever=self.retrievers[first_retriever_name],\n",
    "                                llm=self.gemini_llm\n",
    "                            )\n",
    "                            print(\"Multi-query retriever set up with Gemini\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error setting up multi-query: {e}\")\n",
    "                    else:\n",
    "                        print(\"No Google API key found. Skipping query expansion setup.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error setting up query expansion: {e}\")\n",
    "\n",
    "            print(f\"Set up {len(self.retrievers)} retrievers for hybrid search\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up LangChain retrieval: {e}\")\n",
    "            # Initialize empty retrievers to avoid errors\n",
    "            self.retrievers = {}\n",
    "    \n",
    "    def analyze_query_for_relevance(self, query: str) -> Dict[str, float]:\n",
    "        \"\"\"Analyze query to determine relevant document weights.\"\"\"\n",
    "        # Default to equal weights if no special analysis\n",
    "        doc_ids = set()\n",
    "        for retriever_name in self.retrievers.keys():\n",
    "            if retriever_name.startswith(\"milvus_\"):\n",
    "                parts = retriever_name.split(\"_\")\n",
    "                if len(parts) >= 2:\n",
    "                    doc_ids.add(parts[1])\n",
    "                    \n",
    "        weights = {doc_id: 1.0 for doc_id in doc_ids}\n",
    "        \n",
    "        # If Gemini is available, use it for better weighting\n",
    "        if \"GOOGLE_API_KEY\" in os.environ and os.environ[\"GOOGLE_API_KEY\"]:\n",
    "            try:\n",
    "                gemini_processor = GeminiProcessor()\n",
    "                # Get topic analysis from query\n",
    "                topics = gemini_processor.analyze_query_topics(query)\n",
    "                \n",
    "                # For each document, check metadata for topic relevance\n",
    "                for doc_id in doc_ids:\n",
    "                    metadata = self.vector_db_manager.get_document_metadata(doc_id)\n",
    "                    # Calculate relevance score (basic implementation)\n",
    "                    relevance = 1.0  # default\n",
    "                    if 'title' in metadata:\n",
    "                        # Simple string matching for relevance\n",
    "                        title = metadata['title'].lower()\n",
    "                        for topic in topics:\n",
    "                            if topic.lower() in title:\n",
    "                                relevance += 0.5  # Boost for topic match\n",
    "                    weights[doc_id] = relevance\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing query for document relevance: {e}\")\n",
    "                \n",
    "        return weights\n",
    "\n",
    "    def hybrid_retrieve(self, query: str, k: int = 5):\n",
    "        \"\"\"Perform hybrid retrieval with smart strategies.\"\"\"\n",
    "        all_results = []\n",
    "        active_retrievers = 0\n",
    "        \n",
    "        # Check if there are any retrievers configured\n",
    "        if not self.retrievers:\n",
    "            print(\"No retrievers configured. Returning empty results.\")\n",
    "            return []\n",
    "        \n",
    "        # Analyze query to weight documents by relevance\n",
    "        doc_weights = self.analyze_query_for_relevance(query)\n",
    "            \n",
    "        # Use multi-query retriever if available\n",
    "        if hasattr(self, 'multi_query_retriever'):\n",
    "            try:\n",
    "                print(\"Using multi-query retriever\")\n",
    "                multi_results = self.multi_query_retriever.invoke(query)\n",
    "                all_results.extend(multi_results)\n",
    "                active_retrievers += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error with multi-query retrieval: {e}\")\n",
    "        \n",
    "        # Use all retrievers, weighted by document relevance\n",
    "        for name, retriever in self.retrievers.items():\n",
    "            try:\n",
    "                # Weight results based on document\n",
    "                doc_id = None\n",
    "                if name.startswith(\"milvus_\"):\n",
    "                    parts = name.split(\"_\")\n",
    "                    if len(parts) >= 2:\n",
    "                        doc_id = parts[1]\n",
    "                \n",
    "                # Calculate how many results to retrieve based on weight\n",
    "                retrieval_k = k\n",
    "                if doc_id and doc_id in doc_weights:\n",
    "                    weight = doc_weights[doc_id]\n",
    "                    # Adjust k based on weight (min 1, max 2*k)\n",
    "                    retrieval_k = max(1, min(k*2, round(k * weight)))\n",
    "                \n",
    "                print(f\"Querying retriever: {name} (k={retrieval_k})\")\n",
    "                results = retriever.invoke(query)\n",
    "                \n",
    "                # Add weight to each result's metadata for later sorting\n",
    "                if doc_id and doc_id in doc_weights:\n",
    "                    for doc in results:\n",
    "                        doc.metadata['relevance_weight'] = doc_weights[doc_id]\n",
    "                \n",
    "                all_results.extend(results)\n",
    "                active_retrievers += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving from {name}: {e}\")\n",
    "        \n",
    "        if active_retrievers == 0:\n",
    "            print(\"Warning: No retrievers were successfully queried.\")\n",
    "        \n",
    "        # Smart deduplication and re-ranking\n",
    "        return self._smart_rerank(all_results, query, k)\n",
    "    \n",
    "    def _smart_rerank(self, results: List[Document], query: str, k: int) -> List[Document]:\n",
    "        \"\"\"Smart reranking of results considering duplication, relevance weights and semantic similarity.\"\"\"\n",
    "        if not results:\n",
    "            return []\n",
    "            \n",
    "        # Group similar content using cosine similarity of embeddings\n",
    "        try:\n",
    "            unique_results = []\n",
    "            seen_contents = set()\n",
    "            \n",
    "            # Sort by relevance weight first if available\n",
    "            results = sorted(results, \n",
    "                            key=lambda x: x.metadata.get('relevance_weight', 1.0), \n",
    "                            reverse=True)\n",
    "            \n",
    "            # Use embedding manager to compute query embedding\n",
    "            query_embedding = None\n",
    "            try:\n",
    "                query_embedding = self.vector_db_manager.embedding_manager.generate_embedding(query)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating query embedding: {e}\")\n",
    "            \n",
    "            # Calculate similarity scores if query embedding available\n",
    "            if query_embedding:\n",
    "                for doc in results:\n",
    "                    content_hash = hash(doc.page_content)\n",
    "                    if content_hash not in seen_contents:\n",
    "                        seen_contents.add(content_hash)\n",
    "                        # Try to get or compute document embedding\n",
    "                        if hasattr(doc, 'embedding'):\n",
    "                            doc_embedding = doc.embedding\n",
    "                        else:\n",
    "                            doc_embedding = self.vector_db_manager.embedding_manager.generate_embedding(doc.page_content)\n",
    "                        \n",
    "                        # Calculate similarity score\n",
    "                        similarity = self._cosine_similarity(query_embedding, doc_embedding)\n",
    "                        doc.metadata['similarity_score'] = similarity\n",
    "                        unique_results.append(doc)\n",
    "                \n",
    "                # Final sort by similarity score\n",
    "                unique_results = sorted(unique_results,\n",
    "                                       key=lambda x: x.metadata.get('similarity_score', 0.0),\n",
    "                                       reverse=True)\n",
    "            else:\n",
    "                # Simple deduplication if no embeddings\n",
    "                for doc in results:\n",
    "                    content = doc.page_content\n",
    "                    if content not in seen_contents:\n",
    "                        seen_contents.add(content)\n",
    "                        unique_results.append(doc)\n",
    "            \n",
    "            return unique_results[:k]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in smart reranking: {e}\")\n",
    "            # Fallback to simple deduplication\n",
    "            seen_contents = set()\n",
    "            unique_results = []\n",
    "            for doc in results:\n",
    "                content = doc.page_content\n",
    "                if content not in seen_contents:\n",
    "                    seen_contents.add(content)\n",
    "                    unique_results.append(doc)\n",
    "            return unique_results[:k]\n",
    "    \n",
    "    def _cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        dot_product = sum(a*b for a, b in zip(vec1, vec2))\n",
    "        mag1 = math.sqrt(sum(a*a for a in vec1))\n",
    "        mag2 = math.sqrt(sum(b*b for b in vec2))\n",
    "        if mag1 * mag2 == 0:\n",
    "            return 0\n",
    "        return dot_product / (mag1 * mag2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a230175",
   "metadata": {},
   "source": [
    "### 9. Document Manager\n",
    "The Document Manager will handle the entire lifecycle of documents, including ingestion, processing, and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ade75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentManager:\n",
    "    \"\"\"Class to manage document processing and storage.\"\"\"\n",
    "    \n",
    "    def __init__(self, milvus_host='localhost', milvus_port='19530', \n",
    "                 neo4j_uri='bolt://localhost:7687', neo4j_username='neo4j', neo4j_password='venev'):\n",
    "        \"\"\"Initialize document manager with database connections.\"\"\"\n",
    "        # Initialize components\n",
    "        self.analyzer = DocumentAnalyzer()\n",
    "        self.chunker = DocumentChunker()\n",
    "        self.embedding_manager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Initialize vector database manager\n",
    "        self.vector_db_manager = VectorDBManager(self.embedding_manager)\n",
    "        \n",
    "        # Setup database connections\n",
    "        try:\n",
    "            self.vector_db_manager.setup_milvus(host=milvus_host, port=milvus_port)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to set up Milvus: {e}\")\n",
    "        \n",
    "        try:\n",
    "            self.vector_db_manager.setup_neo4j(uri=neo4j_uri, username=neo4j_username, password=neo4j_password)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to set up Neo4j: {e}\")\n",
    "            \n",
    "        # Create retriever\n",
    "        self.retriever = HybridRetriever(self.vector_db_manager)\n",
    "        self.retriever.setup_langchain_retrieval()\n",
    "        \n",
    "        # Track processed documents with BOTH path and ID\n",
    "        self.processed_documents = set()\n",
    "        self.processed_doc_ids = set()\n",
    "        \n",
    "    def process_document(self, file_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Process a document and store in the databases.\n",
    "        Returns True if processing succeeded.\n",
    "        \"\"\"\n",
    "        # Extract document ID from the file name\n",
    "        doc_id = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "        # Skip if document has already been processed\n",
    "        if file_path in self.processed_documents:\n",
    "            print(f\"Document {file_path} has already been processed\")\n",
    "            return True\n",
    "            \n",
    "        print(f\"Processing document: {file_path}\")\n",
    "        \n",
    "        # Create a pipeline for this document\n",
    "        pipeline = DocumentProcessingPipeline(\n",
    "            analyzer=self.analyzer,\n",
    "            chunker=self.chunker,\n",
    "            embedding_manager=self.embedding_manager,\n",
    "            vector_db_manager=self.vector_db_manager\n",
    "        )\n",
    "        \n",
    "        # Process the document\n",
    "        try:\n",
    "            # Process document with unique identifier\n",
    "            pipeline.process_document(file_path, doc_id=doc_id)\n",
    "            \n",
    "            # Mark as processed - both path and ID\n",
    "            self.processed_documents.add(file_path)\n",
    "            self.processed_doc_ids.add(doc_id)\n",
    "            \n",
    "            # Refresh retriever to include the new document\n",
    "            self.refresh_retriever()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return False\n",
    "        \n",
    "            \n",
    "    def process_multiple_documents(self, file_paths: List[str]) -> List[str]:\n",
    "        \"\"\"Process multiple documents and return list of successfully processed ones.\"\"\"\n",
    "        successful = []\n",
    "        for file_path in file_paths:\n",
    "            if self.process_document(file_path):\n",
    "                successful.append(file_path)\n",
    "        return successful\n",
    "    \n",
    "    def refresh_retriever(self):\n",
    "        \"\"\"Refresh the retriever to recognize new documents.\"\"\"\n",
    "        self.retriever = HybridRetriever(self.vector_db_manager)\n",
    "        self.retriever.setup_langchain_retrieval()\n",
    "        \n",
    "    def query(self, query_text: str, k: int = 5):\n",
    "        \"\"\"Query the system and return documents and LLM response.\"\"\"\n",
    "        documents = self.retriever.hybrid_retrieve(query_text, k=k)\n",
    "        \n",
    "        # Process with LLM if API key is available\n",
    "        if \"GOOGLE_API_KEY\" in os.environ:\n",
    "            gemini_processor = GeminiProcessor()\n",
    "            response = gemini_processor.process_query(query_text, documents)\n",
    "            return documents, response\n",
    "        else:\n",
    "            return documents, None\n",
    "\n",
    "    def list_processed_documents(self):\n",
    "        \"\"\"Return list of processed documents.\"\"\"\n",
    "        return list(self.processed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5c6b5",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1307a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedding model: all-MiniLM-L6-v2\n",
      "Milvus connection parameters configured.\n",
      "Milvus connection test successful\n",
      "Neo4j connection established and schema created.\n",
      "Multi-query retriever set up with Gemini\n",
      "Set up 1 retrievers for hybrid search\n",
      "Processing document: /workspaces/fantastic-engine/knowledge-base/data/sample_pdf.pdf\n",
      "Processing document: /workspaces/fantastic-engine/knowledge-base/data/sample_pdf.pdf (ID: sample_pdf)\n",
      "Neo4j connection established and schema created.\n",
      "Multi-query retriever set up with Gemini\n",
      "Set up 1 retrievers for hybrid search\n",
      "Processing document: /workspaces/fantastic-engine/knowledge-base/data/sample_pdf.pdf\n",
      "Processing document: /workspaces/fantastic-engine/knowledge-base/data/sample_pdf.pdf (ID: sample_pdf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Failed to import jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n",
      "Failed to import jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document analysis complete. Found 206 elements.\n",
      "Identified content types: table, rule_explanation, business_logic, user_scenario\n",
      "Processing tables...\n",
      "Processed batch 1/4\n",
      "Processed batch 1/4\n",
      "Processed batch 2/4\n",
      "Processed batch 3/4\n",
      "Processed batch 2/4\n",
      "Processed batch 3/4\n",
      "Processed batch 4/4\n",
      "Stored triplets in Neo4j (completed batches)\n",
      "Storing table data in Milvus for backup\n",
      "Using sanitized collection name: doc_sample_pdf_business_logic\n",
      "Processed batch 4/4\n",
      "Stored triplets in Neo4j (completed batches)\n",
      "Storing table data in Milvus for backup\n",
      "Using sanitized collection name: doc_sample_pdf_business_logic\n",
      "Stored 176 documents in Milvus collection: doc_sample_pdf_business_logic\n",
      "Processing content type: rule_explanation\n",
      "Created 108 chunks using hierarchical strategy\n",
      "Storing 108 chunks in Milvus\n",
      "Using sanitized collection name: doc_sample_pdf_rule_explanation\n",
      "Stored 176 documents in Milvus collection: doc_sample_pdf_business_logic\n",
      "Processing content type: rule_explanation\n",
      "Created 108 chunks using hierarchical strategy\n",
      "Storing 108 chunks in Milvus\n",
      "Using sanitized collection name: doc_sample_pdf_rule_explanation\n",
      "Stored 108 documents in Milvus collection: doc_sample_pdf_rule_explanation\n",
      "Processing content type: business_logic\n",
      "Created 30 chunks using paragraph strategy\n",
      "Storing 30 chunks in Milvus\n",
      "Using sanitized collection name: doc_sample_pdf_business_logic\n",
      "Stored 108 documents in Milvus collection: doc_sample_pdf_rule_explanation\n",
      "Processing content type: business_logic\n",
      "Created 30 chunks using paragraph strategy\n",
      "Storing 30 chunks in Milvus\n",
      "Using sanitized collection name: doc_sample_pdf_business_logic\n",
      "Stored 30 documents in Milvus collection: doc_sample_pdf_business_logic\n",
      "Processing content type: user_scenario\n",
      "Created 30 chunks using paragraph strategy\n",
      "Storing 30 chunks in Milvus\n",
      "Using sanitized collection name: doc_sample_pdf_user_scenario\n",
      "Stored 30 documents in Milvus collection: doc_sample_pdf_business_logic\n",
      "Processing content type: user_scenario\n",
      "Created 30 chunks using paragraph strategy\n",
      "Storing 30 chunks in Milvus\n",
      "Using sanitized collection name: doc_sample_pdf_user_scenario\n",
      "Stored 30 documents in Milvus collection: doc_sample_pdf_user_scenario\n",
      "Document sample_pdf processing complete.\n",
      "Multi-query retriever set up with Gemini\n",
      "Set up 4 retrievers for hybrid search\n",
      "Successfully processed 1 documents\n",
      "Stored 30 documents in Milvus collection: doc_sample_pdf_user_scenario\n",
      "Document sample_pdf processing complete.\n",
      "Multi-query retriever set up with Gemini\n",
      "Set up 4 retrievers for hybrid search\n",
      "Successfully processed 1 documents\n"
     ]
    }
   ],
   "source": [
    "# Create a document manager\n",
    "doc_manager = DocumentManager(\n",
    "    milvus_host=\"localhost\",\n",
    "    milvus_port=\"19530\", \n",
    "    neo4j_uri=\"bolt://localhost:7687\",\n",
    "    neo4j_username=\"neo4j\",\n",
    "    neo4j_password=\"venev\"\n",
    ")\n",
    "\n",
    "# Process documents (once)\n",
    "doc_paths = [\n",
    "    \"/workspaces/fantastic-engine/knowledge-base/data/sample_pdf.pdf\"\n",
    "]\n",
    "successful = doc_manager.process_multiple_documents(doc_paths)\n",
    "print(f\"Successfully processed {len(successful)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dfbdb7",
   "metadata": {},
   "source": [
    "### Querying the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ded7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini model initialized successfully\n",
      "Using multi-query retriever\n",
      "Using multi-query retriever\n",
      "Querying retriever: milvus_sample_pdf_business_logic (k=5)\n",
      "Querying retriever: milvus_sample_pdf_rule_explanation (k=5)\n",
      "Querying retriever: milvus_sample_pdf_user_scenario (k=5)\n",
      "Querying retriever: neo4j (k=5)\n",
      "Querying retriever: milvus_sample_pdf_business_logic (k=5)\n",
      "Querying retriever: milvus_sample_pdf_rule_explanation (k=5)\n",
      "Querying retriever: milvus_sample_pdf_user_scenario (k=5)\n",
      "Querying retriever: neo4j (k=5)\n",
      "Gemini model initialized successfully\n",
      "Gemini model initialized successfully\n",
      "Retrieved documents:\n",
      "\n",
      "Document 1 [Source: sample_pdf]:\n",
      "4 Refund generated after cancellation communication should be send to net payable amount. (Click\n",
      "\n",
      "\n",
      "Similarity score: 0.6561\n",
      "\n",
      "Document 2 [Source: sample_pdf]:\n",
      "4 Refund generated after cancellation communication should be send to net payable amount. (Click  \n",
      "C...\n",
      "Similarity score: 0.5945\n",
      "\n",
      "Document 3 [Source: sample_pdf]:\n",
      "3) Cancel & rebook Pre cancellation screen should display full refund amount.\n",
      "\n",
      "_-]\n",
      "\n",
      "Commented [AP32...\n",
      "Similarity score: 0.5648\n",
      "\n",
      "Document 4 [Source: sample_pdf]:\n",
      "3) Cancel & rebook Pre cancellation screen should display full refund amount.  \n",
      "_-]  \n",
      "Commented [AP...\n",
      "Similarity score: 0.5648\n",
      "\n",
      "Document 5 [Source: sample_pdf]:\n",
      "11.3.2 Net Premium Refund Amount\n",
      "\n",
      "Refund Amount on Pre cancellation screen should be calculated with...\n",
      "Similarity score: 0.5478\n",
      "\n",
      "\n",
      "LLM Answer:\n",
      "Retrieved documents:\n",
      "\n",
      "Document 1 [Source: sample_pdf]:\n",
      "4 Refund generated after cancellation communication should be send to net payable amount. (Click\n",
      "\n",
      "\n",
      "Similarity score: 0.6561\n",
      "\n",
      "Document 2 [Source: sample_pdf]:\n",
      "4 Refund generated after cancellation communication should be send to net payable amount. (Click  \n",
      "C...\n",
      "Similarity score: 0.5945\n",
      "\n",
      "Document 3 [Source: sample_pdf]:\n",
      "3) Cancel & rebook Pre cancellation screen should display full refund amount.\n",
      "\n",
      "_-]\n",
      "\n",
      "Commented [AP32...\n",
      "Similarity score: 0.5648\n",
      "\n",
      "Document 4 [Source: sample_pdf]:\n",
      "3) Cancel & rebook Pre cancellation screen should display full refund amount.  \n",
      "_-]  \n",
      "Commented [AP...\n",
      "Similarity score: 0.5648\n",
      "\n",
      "Document 5 [Source: sample_pdf]:\n",
      "11.3.2 Net Premium Refund Amount\n",
      "\n",
      "Refund Amount on Pre cancellation screen should be calculated with...\n",
      "Similarity score: 0.5478\n",
      "\n",
      "\n",
      "LLM Answer:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The provided documents describe the refund process for cancellations, but don't specifically differentiate between company-initiated and customer-initiated cancellations in a comprehensive way.  They mention the need for a \"full refund amount\" to be displayed on the pre-cancellation screen (Documents 3, 4, and 5), and that the refund amount should be calculated *without* HR points (Documents 3, 4, and 5).  Document 5 further states that this should apply to both company and customer-initiated cancellations.\n",
       "\n",
       "Document 2 discusses a BAU (Business As Usual) process where refunds might involve a differential amount if full premium plus HR points were paid, but it's unclear how this applies to company-initiated cancellations specifically.\n",
       "\n",
       "Missing information:  A clear and distinct breakdown of the refund calculation process for company and customer-initiated cancellations is not present in the documents.  Details on how the refund amount is actually calculated (beyond the exclusion of HR points) are not given.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results, answer = doc_manager.query(\"Describe the refund flow when both company and customer initiate cancellation.\")\n",
    "\n",
    "# Print document summaries\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nDocument {i+1} [Source: {doc.metadata.get('doc_id', 'unknown')}]:\")\n",
    "    content_preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "    print(content_preview)\n",
    "    \n",
    "    # Print similarity score if available\n",
    "    if 'similarity_score' in doc.metadata:\n",
    "        print(f\"Similarity score: {doc.metadata['similarity_score']:.4f}\")\n",
    "\n",
    "# Format and display LLM answer\n",
    "if answer:\n",
    "    print(\"\\n\\nLLM Answer:\")\n",
    "    from IPython.display import Markdown, display\n",
    "    display(Markdown(answer))\n",
    "else:\n",
    "    print(\"\\n\\nNo LLM answer available - API key may not be configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a3975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82136bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e92ad52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5ea01b4adafd\n",
      "63517eda4fc1\n",
      "63517eda4fc1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm $(docker ps -a -q) -f\n",
    "sudo rm -rf /workspaces/fantastic-engine/knowledge-base/volumes/*\n",
    "sudo rm -rf /workspaces/fantastic-engine/volumes/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ec0484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container neo4j-graph-db  Creating\n",
      " Container neo4j-graph-db  Created\n",
      " Container neo4j-graph-db  Starting\n",
      " Container neo4j-graph-db  Created\n",
      " Container neo4j-graph-db  Starting\n",
      " Container neo4j-graph-db  Started\n",
      " Container neo4j-graph-db  Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait for Milvus Starting...\n",
      "Start successfully.\n",
      "To change the default Milvus configuration, add your settings to the user.yaml file and then restart the service.\n",
      "Start successfully.\n",
      "To change the default Milvus configuration, add your settings to the user.yaml file and then restart the service.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker compose up -d\n",
    "bash /workspaces/fantastic-engine/standalone_embed.sh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c8c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
