Generate proper and efficient solution for this task: Storing a document in vector database.
Instructions:

1. Analyze the document:
Document has headings/titles, text, pargraphs, strikedout text, highlighted text, 
tables and some images in it. Process the document accordingly.

2. After that, The document should be converted into chunks according to following
chunking strategies:
    --> Title + Content chunking (Hierarchical Chunking)
    --> Paragraph-Based Chunking (with line breaks)
    --> Table-Aware Chunking → Neo4j Graph
    --> Metadata-Aware Chunking

POINTS TO BE AVOID DURING CHUNKING:
Fixed sized token chunking: Will randomly split tables and rules.
Blind sentence chunking: May split structured logic and cause hallucination.

3. Finally use a robust embedding model for embedding all these stuff.
Use a model which can be used locally.

4. Use these vector DBs accordingly: Milvus and Neo4j

| Content Type               | Chunking Strategy              | Storage        |
| -------------------------- | ------------------------------ | -------------- |
| Rule explanations          | Title + Content                | Milvus         |
| Business logic flows       | Paragraph-based                | Milvus         |
| Tables                     | Row → triplet (graph chunking) | Neo4j          |
| User scenarios             | Paragraph-based                | Milvus         |
| System interactions        | Paragraph-based                | Neo4j + Milvus |

5. Integrate all these with Langchain (latest version: v0.3) incorporating efficient retrieval techniques.
The retrieval should be Hybrid like combining results from Milvus and Neo4j.

6. I want all this in a jupyter notebook. Use gemini llm model: gemini-1.5-flash-8b via API



GOOGLE_API_KEY="AIzaSyD0YvhLFGBmYiS2KYnuRjG1F-xdxJ2pmO4" python knowledge-base/notebook_export.py